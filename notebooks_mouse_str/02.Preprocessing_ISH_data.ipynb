{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ProjDIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(1, os.path.join(ProjDIR, \"src\"))\n",
    "from ASD_Circuits import (\n",
    "    modify_str, LoadList, ZscoreConverting, quantileNormalize_withNA\n",
    ")\n",
    "\n",
    "os.chdir(os.path.join(ProjDIR, \"notebooks_mouse_str\"))\n",
    "print(f\"Project root: {ProjDIR}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config for data paths\n",
    "with open(os.path.join(ProjDIR, \"config/config.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "ISH_DIR = config[\"data_files\"][\"ish_expression_dir\"]\n",
    "BIAS_DIR = os.path.join(ProjDIR, \"dat/BiasMatrices\")\n",
    "ALLEN_DIR = os.path.join(ProjDIR, \"dat/allen-mouse-exp\")\n",
    "os.makedirs(BIAS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ISH expression dir: {ISH_DIR}\")\n",
    "print(f\"Bias matrices dir:  {BIAS_DIR}\")\n",
    "print(f\"Allen metadata dir: {ALLEN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Preprocessing ISH Data\n",
    "\n",
    "This notebook processes raw Allen ISH expression data into three output matrices:\n",
    "\n",
    "1. **Expression Level Matrix** (Gene × 213 STR) — log2(1+x) of mean expression energy\n",
    "2. **Z1 Matrix** — per-gene z-score of expression across structures\n",
    "3. **Z2 Matrix** — expression-matched z-score (controls for baseline expression level)\n",
    "\n",
    "All matrices are indexed by **human Entrez gene ID** and have 213 brain structure columns.\n",
    "\n",
    "The Z2 computation is parallelized via `scripts/script_compute_Z2.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Load Gene Mappings (from Notebook 01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load human-mouse gene mapping and section ID assignments\n",
    "with open(os.path.join(ALLEN_DIR, \"human2mouse.0420.json\"), \"r\") as f:\n",
    "    Human2Mouse_Genes = json.load(f)\n",
    "with open(os.path.join(ALLEN_DIR, \"mouse2sectionID.0420.json\"), \"r\") as f:\n",
    "    Mouse2Human_Genes = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "print(f\"Human genes with mouse orthologs: {len(Human2Mouse_Genes)}\")\n",
    "print(f\"Mouse genes with section IDs:     {len(Mouse2Human_Genes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 213 selected structures with atlas IDs\n",
    "STR_Meta = pd.read_csv(os.path.join(ALLEN_DIR, \"allen_brain_atlas_structures.csv\"))\n",
    "STR_Meta.dropna(inplace=True, subset=[\"atlas_id\"])\n",
    "STR_Meta[\"atlas_id\"] = STR_Meta[\"atlas_id\"].astype(int)\n",
    "STR_Meta = STR_Meta.set_index(\"atlas_id\")\n",
    "STR_Meta[\"Name2\"] = STR_Meta[\"safe_name\"].apply(modify_str)\n",
    "\n",
    "Selected_STRs = LoadList(os.path.join(ALLEN_DIR, \"Structures.txt\"))\n",
    "STR_Meta_2 = STR_Meta[STR_Meta[\"Name2\"].isin(Selected_STRs)].sort_values(\"Name2\")\n",
    "\n",
    "# Map structure name → structure_id for fast lookup\n",
    "STR_names = STR_Meta_2[\"Name2\"].values\n",
    "STR_ids = STR_Meta_2[\"id\"].values\n",
    "print(f\"Selected structures: {len(STR_Meta_2)} (expected 213)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Build Expression Level Matrix\n",
    "\n",
    "For each human gene:\n",
    "1. Find mouse orthologs → Allen ISH section IDs\n",
    "2. Read each ISH CSV, extract expression energy for each of 213 structures\n",
    "3. Average across all experiments, apply log2(1+x) transform\n",
    "\n",
    "**This reads ~18,000 ISH CSV files. Cached after first run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_mat_path = os.path.join(BIAS_DIR, \"AllenMouseBrain_ExpLevel.parquet\")\n",
    "\n",
    "if os.path.exists(exp_mat_path):\n",
    "    print(f\"Loading cached ExpMat from {exp_mat_path}\")\n",
    "    ExpMat = pd.read_parquet(exp_mat_path)\n",
    "    print(f\"ExpMat shape: {ExpMat.shape}\")\n",
    "else:\n",
    "    # Collect per-gene section IDs\n",
    "    gene_sections = {}  # human_entrez -> list of section_ids\n",
    "    for entrez_str, v in Human2Mouse_Genes.items():\n",
    "        entrez = int(entrez_str) if isinstance(entrez_str, str) else entrez_str\n",
    "        section_ids = []\n",
    "        for m_symbol, m_entrez in v[\"mouseHomo\"]:\n",
    "            if m_entrez in Mouse2Human_Genes:\n",
    "                section_ids.extend(Mouse2Human_Genes[m_entrez][\"allen_section_data_set_id\"])\n",
    "        if len(section_ids) > 0:\n",
    "            gene_sections[entrez] = section_ids\n",
    "\n",
    "    print(f\"Human genes with ISH sections: {len(gene_sections)}\")\n",
    "    print(f\"Total ISH reads needed: {sum(len(s) for s in gene_sections.values())}\")\n",
    "\n",
    "    # Read all ISH CSVs and build expression matrix\n",
    "    All_Genes = []\n",
    "    All_ExpEnergy = []\n",
    "\n",
    "    for i, (entrez, sections) in enumerate(gene_sections.items()):\n",
    "        g_All_dat = []\n",
    "        for section_id in sections:\n",
    "            csv_path = os.path.join(ISH_DIR, f\"{section_id}.csv\")\n",
    "            if not os.path.exists(csv_path):\n",
    "                continue\n",
    "            dat_df = pd.read_csv(csv_path)\n",
    "            # Extract expression energy for each structure\n",
    "            dat = []\n",
    "            for str_id in STR_ids:\n",
    "                match = dat_df[dat_df[\"structure_id\"] == str_id]\n",
    "                if len(match) > 0:\n",
    "                    dat.append(np.log2(1 + match[\"expression_energy\"].values[0]))\n",
    "                else:\n",
    "                    dat.append(np.nan)\n",
    "            g_All_dat.append(dat)\n",
    "\n",
    "        if len(g_All_dat) == 0:\n",
    "            continue\n",
    "\n",
    "        g_All_dat = np.array(g_All_dat)\n",
    "        g_avg = np.nanmean(g_All_dat, axis=0)\n",
    "        All_Genes.append(entrez)\n",
    "        All_ExpEnergy.append(g_avg)\n",
    "\n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(gene_sections)} genes...\")\n",
    "\n",
    "    ExpMat = pd.DataFrame(\n",
    "        data=np.array(All_ExpEnergy),\n",
    "        index=All_Genes,\n",
    "        columns=STR_names\n",
    "    )\n",
    "    ExpMat.index.name = None\n",
    "\n",
    "    # Save\n",
    "    ExpMat.to_parquet(exp_mat_path)\n",
    "    ExpMat.to_csv(\n",
    "        os.path.join(ALLEN_DIR, \"ExpLevel.csv.gz\"),\n",
    "        compression=\"gzip\"\n",
    "    )\n",
    "    print(f\"Saved ExpMat: {ExpMat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "n_valid = ExpMat.dropna(how=\"all\").shape[0]\n",
    "n_nan = ExpMat.isna().sum().sum()\n",
    "print(f\"ExpMat shape:      {ExpMat.shape}\")\n",
    "print(f\"Genes with data:   {n_valid}\")\n",
    "print(f\"Total NaN entries: {n_nan}\")\n",
    "print(f\"Value range:       [{ExpMat.min().min():.3f}, {ExpMat.max().max():.3f}]\")\n",
    "ExpMat.iloc[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 3. Expression Matching\n",
    "\n",
    "Z2 controls for the relationship between baseline expression level and\n",
    "z-score magnitude. To do this, each gene is matched to ~1,000 genes with\n",
    "similar overall expression levels.\n",
    "\n",
    "**Algorithm** (uniform kernel matching):\n",
    "1. Compute **root expression** = mean log2(1+x) expression across all 213 structures\n",
    "2. Rank all genes by root expression, compute quantile (0→1)\n",
    "3. For each gene, find all genes within a **±5% quantile window**\n",
    "4. **Sample 10,000 genes with replacement** from that window (uniform kernel)\n",
    "5. Deduplicate → keep up to 1,000 unique matched genes\n",
    "6. Z2[g,s] = (Z1[g,s] − mean(Z1[matched,s])) / std(Z1[matched,s])\n",
    "\n",
    "The expression feature table and matching are saved for reproducibility.\n",
    "Match files are saved to `dat/allen-mouse-exp/ExpMatch/` (one file per gene)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a. Compute expression feature table (root expression + quantile per gene)\n",
    "exp_features_path = os.path.join(ALLEN_DIR, \"ExpMatchFeatures.csv\")\n",
    "\n",
    "root_exp = ExpMat.mean(axis=1, skipna=True)\n",
    "exp_features = pd.DataFrame({\"Genes\": ExpMat.index, \"EXP\": root_exp.values})\n",
    "exp_features = exp_features.dropna(subset=[\"EXP\"]).reset_index(drop=True)\n",
    "exp_features = exp_features.sort_values(\"EXP\", ascending=True).reset_index(drop=True)\n",
    "exp_features[\"Rank\"] = exp_features.index + 1\n",
    "exp_features[\"quantile\"] = exp_features[\"Rank\"] / len(exp_features)\n",
    "exp_features = exp_features.set_index(\"Genes\")\n",
    "\n",
    "exp_features.to_csv(exp_features_path)\n",
    "print(f\"Expression features: {exp_features.shape[0]} genes\")\n",
    "print(f\"Expression range: [{exp_features['EXP'].min():.3f}, {exp_features['EXP'].max():.3f}]\")\n",
    "print(f\"Saved to: {exp_features_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. Generate expression match files (one per gene)\n",
    "# Each file contains matched gene IDs (within ±5% quantile, sampled with replacement)\n",
    "MATCH_DIR = os.path.join(ALLEN_DIR, \"ExpMatch\")\n",
    "MATCH_SEED = 42\n",
    "MATCH_SAMPLE_SIZE = 10000\n",
    "MATCH_INTERVAL = 0.05\n",
    "\n",
    "existing_matches = len(os.listdir(MATCH_DIR)) if os.path.exists(MATCH_DIR) else 0\n",
    "\n",
    "if existing_matches >= len(exp_features) - 10:\n",
    "    print(f\"Expression match files already exist ({existing_matches} files in {MATCH_DIR})\")\n",
    "else:\n",
    "    os.makedirs(MATCH_DIR, exist_ok=True)\n",
    "    rng = np.random.default_rng(MATCH_SEED)\n",
    "    quantiles = exp_features[\"quantile\"].values\n",
    "    gene_ids = exp_features.index.values\n",
    "\n",
    "    for i, gene in enumerate(gene_ids):\n",
    "        q = quantiles[i]\n",
    "        q_min = max(0, q - MATCH_INTERVAL)\n",
    "        q_max = min(1, q + MATCH_INTERVAL)\n",
    "\n",
    "        mask = (quantiles >= q_min) & (quantiles <= q_max)\n",
    "        mask[i] = False\n",
    "        interval_genes = gene_ids[mask]\n",
    "\n",
    "        if len(interval_genes) == 0:\n",
    "            continue\n",
    "\n",
    "        matched = rng.choice(interval_genes, size=MATCH_SAMPLE_SIZE, replace=True)\n",
    "        with open(os.path.join(MATCH_DIR, f\"{gene}.csv\"), \"w\") as f:\n",
    "            f.write(\"\\n\".join(str(g) for g in matched))\n",
    "\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"  Generated {i + 1}/{len(gene_ids)} match files...\")\n",
    "\n",
    "    n_files = len(os.listdir(MATCH_DIR))\n",
    "    print(f\"Generated {n_files} expression match files in {MATCH_DIR}\")\n",
    "    print(f\"  seed={MATCH_SEED}, sample_size={MATCH_SAMPLE_SIZE}, interval=±{MATCH_INTERVAL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 4. Compute Z1 Matrix (Per-Gene Z-Score)\n",
    "\n",
    "For each gene, z-score its expression values across the 213 structures.\n",
    "This normalizes each gene to have mean=0, std=1 across structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_path = os.path.join(BIAS_DIR, \"AllenMouseBrain_Z1.parquet\")\n",
    "\n",
    "z1_data = []\n",
    "for gene_idx in ExpMat.index:\n",
    "    row = ExpMat.loc[gene_idx].values\n",
    "    z1_data.append(ZscoreConverting(row))\n",
    "\n",
    "Z1Mat = pd.DataFrame(\n",
    "    data=np.array(z1_data),\n",
    "    index=ExpMat.index,\n",
    "    columns=ExpMat.columns\n",
    ")\n",
    "Z1Mat.index.name = None\n",
    "\n",
    "Z1Mat.to_parquet(z1_path)\n",
    "Z1Mat.to_csv(\n",
    "    os.path.join(ALLEN_DIR, \"ExpLevel.log2.Zscore.csv.gz\"),\n",
    "    compression=\"gzip\"\n",
    ")\n",
    "print(f\"Z1 matrix saved: {Z1Mat.shape}\")\n",
    "print(f\"Z1 NaN count: {Z1Mat.isna().sum().sum()}\")\n",
    "print(f\"Z1 range: [{Z1Mat.min().min():.3f}, {Z1Mat.max().max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Compute Z2 Matrix (Expression-Matched Z-Score)\n",
    "\n",
    "For each gene *g* and structure *s*:\n",
    "\n",
    "$$Z2(g, s) = \\frac{Z1(g, s) - \\text{mean}(Z1(\\text{matched}, s))}{\\text{std}(Z1(\\text{matched}, s))}$$\n",
    "\n",
    "**Expression matching**: For each gene, sample 10,000 genes (with replacement)\n",
    "from a ±5% quantile window around its root expression level (uniform kernel).\n",
    "\n",
    "Computation is parallelized via `scripts/script_compute_Z2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2_path = os.path.join(BIAS_DIR, \"AllenMouseBrain_Z2bias.parquet\")\n",
    "z2_csv_path = os.path.join(ALLEN_DIR, \"AllenMouseBrain_Z2bias.csv.gz\")\n",
    "\n",
    "if os.path.exists(z2_path) and os.path.getmtime(z2_path) > os.path.getmtime(z1_path):\n",
    "    print(f\"Loading cached Z2 from {z2_path}\")\n",
    "    Z2Mat = pd.read_parquet(z2_path)\n",
    "    print(f\"Z2 shape: {Z2Mat.shape}\")\n",
    "else:\n",
    "    print(\"Computing Z2 via scripts/script_compute_Z2.py ...\")\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        os.path.join(ProjDIR, \"scripts/script_compute_Z2.py\"),\n",
    "        \"--z1\", z1_path,\n",
    "        \"--exp-features\", exp_features_path,\n",
    "        \"--output\", z2_path,\n",
    "        \"--seed\", \"42\",\n",
    "        \"--n-jobs\", \"10\",\n",
    "        \"--also-csv\",\n",
    "        \"--match-dir\", MATCH_DIR,\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        raise RuntimeError(f\"Z2 computation failed with exit code {result.returncode}\")\n",
    "\n",
    "    Z2Mat = pd.read_parquet(z2_path)\n",
    "    print(f\"Z2 matrix loaded: {Z2Mat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Z2 summary:\")\n",
    "print(f\"  Shape:     {Z2Mat.shape}\")\n",
    "print(f\"  NaN count: {Z2Mat.isna().sum().sum()}\")\n",
    "print(f\"  Range:     [{Z2Mat.min().min():.4f}, {Z2Mat.max().max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 6. Comparison with Reference Z2\n",
    "\n",
    "Compare the newly computed Z2 matrix against the old reference to verify\n",
    "consistency. Due to different random seeds in expression matching, values\n",
    "will not be identical but should be highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Try backup parquet first (saved before overwrite), then csv.gz\n",
    "ref_parquet = os.path.join(BIAS_DIR, \"AllenMouseBrain_Z2bias.parquet.bak\")\n",
    "ref_csv = os.path.join(ALLEN_DIR, \"AllenMouseBrain_Z2bias.csv.gz\")\n",
    "\n",
    "if os.path.exists(ref_parquet):\n",
    "    Z2_ref = pd.read_parquet(ref_parquet)\n",
    "    print(f\"Reference Z2 (parquet backup) shape: {Z2_ref.shape}\")\n",
    "elif os.path.exists(ref_csv):\n",
    "    Z2_ref = pd.read_csv(ref_csv, index_col=0)\n",
    "    print(f\"Reference Z2 (csv.gz) shape: {Z2_ref.shape}\")\n",
    "else:\n",
    "    Z2_ref = None\n",
    "    print(\"No reference Z2 found for comparison\")\n",
    "if Z2_ref is not None:\n",
    "    print(f\"New Z2 shape: {Z2Mat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Z2_ref is not None:\n",
    "    # Align on common genes and structures\n",
    "    common_g = Z2Mat.index.intersection(Z2_ref.index)\n",
    "    common_s = Z2Mat.columns.intersection(Z2_ref.columns)\n",
    "    print(f\"Common genes:      {len(common_g)}\")\n",
    "    print(f\"Common structures: {len(common_s)}\")\n",
    "\n",
    "    z2_new = Z2Mat.loc[common_g, common_s].values.flatten()\n",
    "    z2_old = Z2_ref.loc[common_g, common_s].values.flatten()\n",
    "\n",
    "    # Remove NaN pairs\n",
    "    valid = ~(np.isnan(z2_new) | np.isnan(z2_old))\n",
    "    z2_new_v = z2_new[valid]\n",
    "    z2_old_v = z2_old[valid]\n",
    "\n",
    "    r_pearson, p_pearson = pearsonr(z2_new_v, z2_old_v)\n",
    "    r_spearman, p_spearman = spearmanr(z2_new_v, z2_old_v)\n",
    "    mae = np.mean(np.abs(z2_new_v - z2_old_v))\n",
    "    max_diff = np.max(np.abs(z2_new_v - z2_old_v))\n",
    "\n",
    "    print(f\"\\nZ2 Comparison (new vs old reference):\")\n",
    "    print(f\"  Valid value pairs: {len(z2_new_v)}\")\n",
    "    print(f\"  Pearson r:         {r_pearson:.6f}\")\n",
    "    print(f\"  Spearman r:        {r_spearman:.6f}\")\n",
    "    print(f\"  Mean abs error:    {mae:.4f}\")\n",
    "    print(f\"  Max abs error:     {max_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Z2_ref is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.patch.set_alpha(0)\n",
    "\n",
    "    # Scatter plot\n",
    "    ax = axes[0]\n",
    "    idx = np.random.default_rng(0).choice(len(z2_new_v), size=min(50000, len(z2_new_v)), replace=False)\n",
    "    ax.scatter(z2_old_v[idx], z2_new_v[idx], s=0.5, alpha=0.3)\n",
    "    ax.plot([-8, 20], [-8, 20], \"r--\", lw=0.5)\n",
    "    ax.set_xlabel(\"Reference Z2\")\n",
    "    ax.set_ylabel(\"New Z2\")\n",
    "    ax.set_title(f\"Z2 Comparison (r={r_pearson:.4f})\")\n",
    "    ax.patch.set_alpha(0)\n",
    "\n",
    "    # Per-structure correlation\n",
    "    ax = axes[1]\n",
    "    str_corrs = []\n",
    "    for s in common_s:\n",
    "        old_s = Z2_ref.loc[common_g, s].values\n",
    "        new_s = Z2Mat.loc[common_g, s].values\n",
    "        valid_s = ~(np.isnan(old_s) | np.isnan(new_s))\n",
    "        if valid_s.sum() > 10:\n",
    "            r, _ = pearsonr(old_s[valid_s], new_s[valid_s])\n",
    "            str_corrs.append(r)\n",
    "    ax.hist(str_corrs, bins=30, edgecolor=\"black\")\n",
    "    ax.axvline(np.mean(str_corrs), color=\"red\", linestyle=\"--\",\n",
    "               label=f\"mean={np.mean(str_corrs):.4f}\")\n",
    "    ax.set_xlabel(\"Pearson r (per structure)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Per-Structure Correlation\")\n",
    "    ax.legend()\n",
    "    ax.patch.set_alpha(0)\n",
    "\n",
    "    # Difference distribution\n",
    "    ax = axes[2]\n",
    "    diffs = z2_new_v - z2_old_v\n",
    "    ax.hist(diffs, bins=100, edgecolor=\"black\")\n",
    "    ax.axvline(0, color=\"red\", linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Z2_new - Z2_old\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Difference Distribution (MAE={mae:.4f})\")\n",
    "    ax.patch.set_alpha(0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(ProjDIR, \"results/figures\"), exist_ok=True)\n",
    "    plt.savefig(\n",
    "        os.path.join(ProjDIR, \"results/figures/Z2_comparison.png\"),\n",
    "        dpi=150, transparent=True, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 7. Check Downstream Impact\n",
    "\n",
    "Compute ASD bias using both the new and old Z2 matrices to verify that\n",
    "downstream results are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Z2_ref is not None:\n",
    "    from ASD_Circuits import MouseSTR_AvgZ_Weighted, Fil2Dict\n",
    "\n",
    "    # Load ASD gene weights\n",
    "    gw_path = os.path.join(ProjDIR, \"dat/Genetics/GeneWeights/Spark_Meta_EWS.GeneWeight.csv\")\n",
    "    Gene2Weights = Fil2Dict(gw_path)\n",
    "\n",
    "    # Compute bias with new Z2\n",
    "    bias_new = MouseSTR_AvgZ_Weighted(Z2Mat, Gene2Weights)\n",
    "    bias_old = MouseSTR_AvgZ_Weighted(Z2_ref, Gene2Weights)\n",
    "\n",
    "    # Compare\n",
    "    common_str = bias_new.index.intersection(bias_old.index)\n",
    "    r_bias, _ = pearsonr(\n",
    "        bias_new.loc[common_str, \"EFFECT\"].values,\n",
    "        bias_old.loc[common_str, \"EFFECT\"].values\n",
    "    )\n",
    "    bias_diff = (bias_new.loc[common_str, \"EFFECT\"] - bias_old.loc[common_str, \"EFFECT\"]).abs()\n",
    "\n",
    "    print(f\"ASD Bias Comparison (new vs old Z2):\")\n",
    "    print(f\"  Structures:       {len(common_str)}\")\n",
    "    print(f\"  Pearson r:        {r_bias:.6f}\")\n",
    "    print(f\"  Mean abs diff:    {bias_diff.mean():.6f}\")\n",
    "    print(f\"  Max abs diff:     {bias_diff.max():.6f}\")\n",
    "\n",
    "    # Show top structures\n",
    "    top_new = bias_new.sort_values(\"EFFECT\", ascending=False).head(10)\n",
    "    top_old = bias_old.sort_values(\"EFFECT\", ascending=False).head(10)\n",
    "    print(f\"\\nTop 10 by new Z2:  {top_new.index.tolist()}\")\n",
    "    print(f\"Top 10 by old Z2:  {top_old.index.tolist()}\")\n",
    "    print(f\"Overlap:           {len(set(top_new.index) & set(top_old.index))}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Output | Shape | Path |\n",
    "|--------|-------|------|\n",
    "| Expression Level | Gene × 213 STR | `dat/BiasMatrices/AllenMouseBrain_ExpLevel.parquet` |\n",
    "| Z1 (z-score) | Gene × 213 STR | `dat/BiasMatrices/AllenMouseBrain_Z1.parquet` |\n",
    "| Z2 (exp-matched) | Gene × 213 STR | `dat/BiasMatrices/AllenMouseBrain_Z2bias.parquet` |"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "gencic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

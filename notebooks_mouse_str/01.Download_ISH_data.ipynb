{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import requests\n",
    "import json\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ProjDIR = \"/home/jw3514/Work/ASD_Circuits_CellType\"\n",
    "sys.path.insert(1, os.path.join(ProjDIR, \"src\"))\n",
    "from ASD_Circuits import modify_str, LoadList\n",
    "\n",
    "os.chdir(os.path.join(ProjDIR, \"notebooks_mouse_str\"))\n",
    "print(f\"Project root: {ProjDIR}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config for data paths\n",
    "with open(os.path.join(ProjDIR, \"config/config.yaml\"), \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "ISH_DIR = config[\"data_files\"][\"ish_expression_dir\"]\n",
    "print(f\"ISH expression directory: {ISH_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Download ISH Data and Build Expression Matrix\n",
    "\n",
    "This notebook downloads input data from the Allen Brain Atlas and builds the\n",
    "gene expression matrix used for all structure-level analyses.\n",
    "\n",
    "1. **Allen ISH experiment metadata** — list of all mouse brain ISH experiments with gene info\n",
    "2. **Allen gene catalog** — all genes with Entrez IDs in the Allen Mouse Brain Atlas\n",
    "3. **ISH expression energy** — per-structure unionized expression for each experiment (bulk download)\n",
    "4. **Human-Mouse gene homology** — ortholog mapping from MGI/JAX\n",
    "5. **HGNC ortholog mappings** — human↔mouse gene mappings via homology groups\n",
    "6. **Discontinued mouse Entrez IDs** — update Allen data with current Entrez IDs\n",
    "7. **Human gene → Allen section IDs** — union of Entrez-based and symbol-based routes (Jon's strategy)\n",
    "8. **Structure metadata** — 213 selected brain structures\n",
    "9. **Expression matrix** — arithmetic mean of raw expression energy across ISH sections per gene\n",
    "\n",
    "All outputs are saved to `dat/allen-mouse-exp/`.\n",
    "Data downloads are guarded by `os.path.exists()` checks to avoid re-downloading.\n",
    "\n",
    "### Gene mapping strategy\n",
    "\n",
    "Following Jon's R pipeline (`human_gene.R`), each human gene is mapped to\n",
    "Allen ISH section IDs via **two independent routes**, then the results are\n",
    "**unioned**:\n",
    "\n",
    "- **Entrez route**: human Entrez → mouse Entrez (HGNC) → Allen section IDs\n",
    "- **Symbol route**: human Entrez → human symbol → mouse symbol (HGNC) → Allen section IDs\n",
    "\n",
    "Before mapping, discontinued mouse Entrez IDs in the Allen data are updated\n",
    "to their current equivalents (414 sections affected, 253 mouse genes recovered).\n",
    "See `docs/Jon_Exp_Mat.md` for full documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Allen ISH Experiment Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download list of all ISH experiments with gene and plane-of-section information\n",
    "out_path = os.path.join(ProjDIR, \"dat/allen-mouse-exp/All_Mouse_Brain_ISH_experiments.csv\")\n",
    "if not os.path.exists(out_path):\n",
    "    url = (\n",
    "        \"http://api.brain-map.org/api/v2/data/query.csv?criteria=model::SectionDataSet,\"\n",
    "        \"rma::criteria,[failed$eqfalse],products[abbreviation$eq'Mouse'],\"\n",
    "        \"treatments[name$eq'ISH'],genes,plane_of_section,\"\n",
    "        \"rma::options,[tabular$eq'plane_of_sections.name+as+plane',\"\n",
    "        \"'genes.acronym+as+gene_acronym',\"\n",
    "        \"'genes.entrez_id+as+genes_entrez_id',\"\n",
    "        \"'genes.ensembl_id+as+gene_ensembl_id',\"\n",
    "        \"'genes.alias_tags+as+gene_alias_tags',\"\n",
    "        \"'data_sets.id+as+section_data_set_id'],\"\n",
    "        \"[order$eq'plane_of_sections.name,genes.acronym,data_sets.id']&\"\n",
    "        \"start_row=0&num_rows=all\"\n",
    "    )\n",
    "    print(f\"Downloading ISH experiments from:\\n{url}\")\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Saved to {out_path}\")\n",
    "else:\n",
    "    print(f\"Already exists: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download complete gene catalog (gene symbols, aliases, Entrez IDs)\n",
    "out_path = os.path.join(ProjDIR, \"dat/allen-mouse-exp/All_Mouse_Brain_ISH_genes.csv\")\n",
    "if not os.path.exists(out_path):\n",
    "    url = (\n",
    "        \"http://api.brain-map.org/api/v2/data/query.csv?criteria=model::Gene,\"\n",
    "        \"rma::criteria,[failed$eqfalse],products[abbreviation$eq'Mouse'],\"\n",
    "        \"[tabular$eq'acronym+as+gene_acronym',\"\n",
    "        \"'gene_aliases.name+as+gene_aliases',\"\n",
    "        \"'entrez_id+as+genes_entrez_id',\"\n",
    "        \"[order$eq'acronym']&\"\n",
    "        \"start_row=0&num_rows=all\"\n",
    "    )\n",
    "    print(f\"Downloading gene catalog from:\\n{url}\")\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Saved to {out_path}\")\n",
    "else:\n",
    "    print(f\"Already exists: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download gene metadata with homologene IDs\n",
    "out_path = os.path.join(ProjDIR, \"dat/allen-mouse-exp/All_Mouse_Brain_Genes.csv\")\n",
    "if not os.path.exists(out_path):\n",
    "    url = (\n",
    "        \"http://api.brain-map.org/api/v2/data/query.csv?criteria=\"\n",
    "        \"model::Gene,\"\n",
    "        \"rma::criteria,products[abbreviation$eq'Mouse'],\"\n",
    "        \"rma::options,[tabular$eq'genes.id','genes.acronym+as+gene_symbol',\"\n",
    "        \"'genes.name+as+gene_name','genes.entrez_id+as+entrez_gene_id',\"\n",
    "        \"'genes.homologene_id+as+homologene_group_id',\"\n",
    "        \"'genes.alias_tags+as+gene_alias_tags'],\"\n",
    "        \"[order$eq'genes.acronym']&num_rows=all&start_row=0\"\n",
    "    )\n",
    "    print(f\"Downloading gene metadata from:\\n{url}\")\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Saved to {out_path}\")\n",
    "else:\n",
    "    print(f\"Already exists: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download ISH Expression Energy (Bulk)\n",
    "\n",
    "Each ISH experiment has a `section_data_set_id`. For each, we download the\n",
    "structure-unionized expression energy from the Allen API. This produces one\n",
    "CSV per experiment in the ISH directory configured in `config/config.yaml`.\n",
    "\n",
    "**This is a large download (~26,000 files). Skip if already present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_PATH = \"http://api.brain-map.org/api/v2/data\"\n",
    "GRAPH_ID = 1\n",
    "\n",
    "experiments_csv = os.path.join(ProjDIR, \"dat/allen-mouse-exp/All_Mouse_Brain_ISH_experiments.csv\")\n",
    "\n",
    "# Check how many are already downloaded\n",
    "existing_files = set(os.listdir(ISH_DIR)) if os.path.exists(ISH_DIR) else set()\n",
    "print(f\"ISH directory: {ISH_DIR}\")\n",
    "print(f\"Existing expression files: {len(existing_files)}\")\n",
    "\n",
    "if len(existing_files) < 25000:\n",
    "    os.makedirs(ISH_DIR, exist_ok=True)\n",
    "    experiments = pd.read_csv(experiments_csv)\n",
    "    section_ids = experiments[\"section_data_set_id\"].unique()\n",
    "    print(f\"Total unique section IDs: {len(section_ids)}\")\n",
    "\n",
    "    url_base = (\n",
    "        f\"{API_PATH}/StructureUnionize/query.csv\"\n",
    "        f\"?criteria=[section_data_set_id$eq%d]\"\n",
    "        f\",structure[graph_id$eq{GRAPH_ID}]\"\n",
    "        f\"&numRows=all\"\n",
    "    )\n",
    "    n_downloaded = 0\n",
    "    for i, section_id in enumerate(section_ids):\n",
    "        fname = f\"{section_id}.csv\"\n",
    "        if fname in existing_files:\n",
    "            continue\n",
    "        url = url_base % section_id\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open(os.path.join(ISH_DIR, fname), \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        n_downloaded += 1\n",
    "        if n_downloaded % 1000 == 0:\n",
    "            print(f\"  Downloaded {n_downloaded} / {len(section_ids) - len(existing_files)} remaining...\")\n",
    "    print(f\"Downloaded {n_downloaded} new files. Total: {len(existing_files) + n_downloaded}\")\n",
    "else:\n",
    "    print(f\"ISH expression data already downloaded ({len(existing_files)} files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Human-Mouse Gene Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MGI Human-Mouse homology report\n",
    "hom_path = os.path.join(ProjDIR, \"dat/HOM_MouseHumanSequence.rpt\")\n",
    "if not os.path.exists(hom_path):\n",
    "    url = \"https://www.informatics.jax.org/downloads/reports/HOM_MouseHumanSequence.rpt\"\n",
    "    print(f\"Downloading homology data from:\\n{url}\")\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    with open(hom_path, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(f\"Saved to {hom_path}\")\n",
    "else:\n",
    "    print(f\"Already exists: {hom_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 4. Load Data for Gene Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllenMouseGenes = pd.read_csv(\n",
    "    os.path.join(ProjDIR, \"dat/allen-mouse-exp/All_Mouse_Brain_ISH_experiments.csv\")\n",
    ")\n",
    "Human2MouseHom = pd.read_csv(hom_path, delimiter=\"\\t\")\n",
    "print(f\"Allen ISH experiments: {len(AllenMouseGenes)}\")\n",
    "print(f\"Homology entries: {len(Human2MouseHom)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. Build HGNC Ortholog Mappings\n",
    "\n",
    "Three mappings needed for Jon's union strategy:\n",
    "- human Entrez → mouse Entrez (via DB Class Key homology groups)\n",
    "- human symbol → mouse symbol (via DB Class Key homology groups)\n",
    "- human Entrez → human symbol (for the symbol route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_dat = Human2MouseHom[Human2MouseHom[\"NCBI Taxon ID\"] == 9606]\n",
    "mus_dat = Human2MouseHom[Human2MouseHom[\"NCBI Taxon ID\"] == 10090]\n",
    "\n",
    "# human Entrez → mouse Entrez\n",
    "# For each human gene, find its homology groups, expand to related human genes,\n",
    "# then find all mouse genes in those groups (Jon: mapl$human.entrez.to.mouse.entrez)\n",
    "human_entrez_to_mouse_entrez = {}\n",
    "for hentrez in hum_dat[\"EntrezGene ID\"].unique():\n",
    "    db_keys = set(hum_dat[hum_dat[\"EntrezGene ID\"] == hentrez][\"DB Class Key\"])\n",
    "    related_human = set(hum_dat[hum_dat[\"DB Class Key\"].isin(db_keys)][\"EntrezGene ID\"])\n",
    "    all_db_keys = set(hum_dat[hum_dat[\"EntrezGene ID\"].isin(related_human)][\"DB Class Key\"])\n",
    "    mouse_genes = set(mus_dat[mus_dat[\"DB Class Key\"].isin(all_db_keys)][\"EntrezGene ID\"])\n",
    "    if mouse_genes:\n",
    "        human_entrez_to_mouse_entrez[int(hentrez)] = [int(m) for m in mouse_genes]\n",
    "\n",
    "# human symbol → mouse symbol (Jon: mapl$human.symbol.to.mouse.symbol)\n",
    "human_symbol_to_mouse_symbol = {}\n",
    "for hsym in hum_dat[\"Symbol\"].unique():\n",
    "    db_keys = set(hum_dat[hum_dat[\"Symbol\"] == hsym][\"DB Class Key\"])\n",
    "    related_human = set(hum_dat[hum_dat[\"DB Class Key\"].isin(db_keys)][\"Symbol\"])\n",
    "    all_db_keys = set(hum_dat[hum_dat[\"Symbol\"].isin(related_human)][\"DB Class Key\"])\n",
    "    mouse_syms = set(mus_dat[mus_dat[\"DB Class Key\"].isin(all_db_keys)][\"Symbol\"])\n",
    "    if mouse_syms:\n",
    "        human_symbol_to_mouse_symbol[hsym] = list(mouse_syms)\n",
    "\n",
    "# human Entrez → human symbol (Jon uses org.Hs.eg.db; we use HGNC directly)\n",
    "human_entrez_to_symbol = {}\n",
    "for _, row in hum_dat[[\"EntrezGene ID\", \"Symbol\"]].drop_duplicates().iterrows():\n",
    "    eid = int(row[\"EntrezGene ID\"])\n",
    "    human_entrez_to_symbol.setdefault(eid, []).append(row[\"Symbol\"])\n",
    "\n",
    "print(f\"human Entrez → mouse Entrez: {len(human_entrez_to_mouse_entrez)} genes\")\n",
    "print(f\"human symbol → mouse symbol: {len(human_symbol_to_mouse_symbol)} genes\")\n",
    "print(f\"human Entrez → human symbol: {len(human_entrez_to_symbol)} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pickle files for backward compatibility\n",
    "Mouse2Human_Genes = {}\n",
    "Human2Mouse_Genes = {}\n",
    "Homo_IDs = set(Human2MouseHom[\"DB Class Key\"].values)\n",
    "\n",
    "for ID in Homo_IDs:\n",
    "    tmp_df = Human2MouseHom[Human2MouseHom[\"DB Class Key\"] == ID]\n",
    "    hum_genes, mou_genes = [], []\n",
    "    for _, row in tmp_df.iterrows():\n",
    "        TaxonID = row[\"NCBI Taxon ID\"]\n",
    "        Symbol = row[\"Symbol\"]\n",
    "        Entrez = row[\"EntrezGene ID\"]\n",
    "        if TaxonID == 9606:\n",
    "            hum_genes.append((Symbol, Entrez))\n",
    "        elif TaxonID == 10090:\n",
    "            mou_genes.append((Symbol, Entrez))\n",
    "\n",
    "    for (Symbol, Entrez) in hum_genes:\n",
    "        if Entrez not in Human2Mouse_Genes:\n",
    "            Human2Mouse_Genes[Entrez] = {\"symbol\": Symbol, \"mouseHomo\": mou_genes}\n",
    "        else:\n",
    "            Human2Mouse_Genes[Entrez][\"mouseHomo\"].extend(mou_genes)\n",
    "\n",
    "    for (Symbol, Entrez) in mou_genes:\n",
    "        if Entrez not in Mouse2Human_Genes:\n",
    "            Mouse2Human_Genes[Entrez] = {\"symbol\": Symbol, \"humanHomo\": hum_genes}\n",
    "        else:\n",
    "            Mouse2Human_Genes[Entrez][\"humanHomo\"].extend(hum_genes)\n",
    "\n",
    "Mouse2Human_Genes_2 = {}\n",
    "for k, v in Mouse2Human_Genes.items():\n",
    "    Mouse2Human_Genes_2[v[\"symbol\"]] = {\"Entrez\": k, \"humanHomo\": v[\"humanHomo\"]}\n",
    "\n",
    "pk.dump(Mouse2Human_Genes_2, open(os.path.join(ProjDIR, \"dat/Mouse2Human_Symbol.pk\"), \"wb\"))\n",
    "pk.dump(Mouse2Human_Genes, open(os.path.join(ProjDIR, \"dat/Mouse2Human_Entrez.pk\"), \"wb\"))\n",
    "print(f\"Human genes with mouse orthologs: {len(Human2Mouse_Genes)}\")\n",
    "print(f\"Mouse genes with human orthologs: {len(Mouse2Human_Genes)}\")\n",
    "print(\"Saved Mouse2Human_Symbol.pk and Mouse2Human_Entrez.pk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 6. Update Discontinued Mouse Entrez IDs in Allen Data\n",
    "\n",
    "Allen ISH experiments reference mouse Entrez IDs that may be discontinued.\n",
    "Before mapping, update these to current IDs (Jon: `download.R` + `human_gene.R`).\n",
    "This recovers ~414 sections across ~253 mouse genes that would otherwise be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disc = pd.read_csv(os.path.join(ProjDIR, \"dat/gene_history.human.mouse.tsv\"), delimiter=\"\\t\")\n",
    "\n",
    "# Build discontinued → current mapping for mouse genes (tax_id=10090)\n",
    "disc_mouse = df_disc[(df_disc[\"#tax_id\"] == 10090) & (df_disc[\"GeneID\"] != \"-\")].copy()\n",
    "disc_mouse[\"GeneID\"] = disc_mouse[\"GeneID\"].astype(int)\n",
    "disc_mouse[\"Discontinued_GeneID\"] = disc_mouse[\"Discontinued_GeneID\"].astype(int)\n",
    "disc_mouse_map = dict(zip(disc_mouse[\"Discontinued_GeneID\"], disc_mouse[\"GeneID\"]))\n",
    "\n",
    "# Resolve chains: if A→B and B→C, then A→C (Jon's while-loop logic)\n",
    "changed = True\n",
    "while changed:\n",
    "    changed = False\n",
    "    for old_id, new_id in list(disc_mouse_map.items()):\n",
    "        if new_id in disc_mouse_map:\n",
    "            disc_mouse_map[old_id] = disc_mouse_map[new_id]\n",
    "            changed = True\n",
    "\n",
    "print(f\"Discontinued mouse Entrez mappings: {len(disc_mouse_map)} (chain-resolved)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Allen experiment table with current Entrez IDs\n",
    "allen_work = AllenMouseGenes[AllenMouseGenes[\"genes_entrez_id\"].notna()].copy()\n",
    "allen_work[\"genes_entrez_id\"] = allen_work[\"genes_entrez_id\"].astype(int)\n",
    "\n",
    "n_updated = allen_work[\"genes_entrez_id\"].isin(disc_mouse_map.keys()).sum()\n",
    "allen_work[\"genes_entrez_id\"] = allen_work[\"genes_entrez_id\"].map(\n",
    "    lambda x: disc_mouse_map.get(x, x)\n",
    ")\n",
    "print(f\"Updated {n_updated} Allen section entries (discontinued → current Entrez)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 7. Map Human Genes → Allen Section IDs (Union Strategy)\n",
    "\n",
    "Following Jon's `human_gene.R`, we map each human gene to Allen ISH\n",
    "section IDs via **two independent routes**, then **union** the results:\n",
    "\n",
    "1. **Entrez route**: human Entrez → mouse Entrez → section IDs\n",
    "2. **Symbol route**: human Entrez → human symbol → mouse symbol → section IDs\n",
    "\n",
    "This union approach recovers additional sections for genes where the Allen\n",
    "database uses different identifiers (e.g., deprecated gene symbols) across\n",
    "the two mapping paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mouse gene → Allen section ID maps from updated Allen data\n",
    "mouse_entrez_to_sections = {}\n",
    "for eid, group in allen_work.groupby(\"genes_entrez_id\"):\n",
    "    mouse_entrez_to_sections[int(eid)] = sorted(set(group[\"section_data_set_id\"]))\n",
    "\n",
    "mouse_symbol_to_sections = {}\n",
    "for sym, group in allen_work.groupby(\"gene_acronym\"):\n",
    "    mouse_symbol_to_sections[sym] = sorted(set(group[\"section_data_set_id\"]))\n",
    "\n",
    "print(f\"Mouse Entrez → sections: {len(mouse_entrez_to_sections)} genes\")\n",
    "print(f\"Mouse symbol → sections: {len(mouse_symbol_to_sections)} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrez route: human Entrez → mouse Entrez → sections\n",
    "human_sections_entrez = {}\n",
    "for hentrez, mouse_list in human_entrez_to_mouse_entrez.items():\n",
    "    sections = []\n",
    "    for mentrez in mouse_list:\n",
    "        sections.extend(mouse_entrez_to_sections.get(mentrez, []))\n",
    "    if sections:\n",
    "        human_sections_entrez[hentrez] = sorted(set(sections))\n",
    "\n",
    "# Symbol route: human Entrez → human symbol → mouse symbol → sections\n",
    "# Step 1: human symbol → mouse symbol → sections\n",
    "hsym_to_sections = {}\n",
    "for hsym, mouse_syms in human_symbol_to_mouse_symbol.items():\n",
    "    sections = []\n",
    "    for msym in mouse_syms:\n",
    "        sections.extend(mouse_symbol_to_sections.get(msym, []))\n",
    "    if sections:\n",
    "        hsym_to_sections[hsym] = sorted(set(sections))\n",
    "\n",
    "# Step 2: human Entrez → human symbol → sections\n",
    "human_sections_symbol = {}\n",
    "for hentrez, symbols in human_entrez_to_symbol.items():\n",
    "    sections = []\n",
    "    for sym in symbols:\n",
    "        sections.extend(hsym_to_sections.get(sym, []))\n",
    "    if sections:\n",
    "        human_sections_symbol[hentrez] = sorted(set(sections))\n",
    "\n",
    "# Union both routes\n",
    "all_human_genes = sorted(set(human_sections_entrez.keys()) | set(human_sections_symbol.keys()))\n",
    "gene_sections = {}\n",
    "for hentrez in all_human_genes:\n",
    "    ent_secs = set(human_sections_entrez.get(hentrez, []))\n",
    "    sym_secs = set(human_sections_symbol.get(hentrez, []))\n",
    "    combined = sorted(ent_secs | sym_secs)\n",
    "    if combined:\n",
    "        gene_sections[hentrez] = combined\n",
    "\n",
    "# Count contributions from each route\n",
    "n_only_entrez = len(set(human_sections_entrez) - set(human_sections_symbol))\n",
    "n_only_symbol = len(set(human_sections_symbol) - set(human_sections_entrez))\n",
    "n_both = len(set(human_sections_entrez) & set(human_sections_symbol))\n",
    "n_extra = sum(\n",
    "    1 for g in set(human_sections_entrez) & set(human_sections_symbol)\n",
    "    if set(human_sections_symbol.get(g, [])) - set(human_sections_entrez.get(g, []))\n",
    ")\n",
    "\n",
    "print(f\"\\nEntrez route: {len(human_sections_entrez)} genes\")\n",
    "print(f\"Symbol route: {len(human_sections_symbol)} genes\")\n",
    "print(f\"Union (final): {len(gene_sections)} genes\")\n",
    "print(f\"  Only via Entrez: {n_only_entrez}\")\n",
    "print(f\"  Only via symbol: {n_only_symbol}\")\n",
    "print(f\"  Both routes: {n_both} ({n_extra} gain extra sections from union)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save gene mapping\n",
    "DIR = os.path.join(ProjDIR, \"dat/allen-mouse-exp/\")\n",
    "\n",
    "with open(os.path.join(DIR, \"human2mouse.0420.json\"), \"w\") as f:\n",
    "    json.dump(Human2Mouse_Genes, f)\n",
    "\n",
    "# Save human gene → section ID mapping (used by section 9)\n",
    "with open(os.path.join(DIR, \"human_gene_sections.json\"), \"w\") as f:\n",
    "    json.dump(gene_sections, f)\n",
    "\n",
    "print(f\"Saved human2mouse.0420.json ({len(Human2Mouse_Genes)} entries)\")\n",
    "print(f\"Saved human_gene_sections.json ({len(gene_sections)} entries)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 8. Structure Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_Meta = pd.read_csv(os.path.join(ProjDIR, \"dat/allen-mouse-exp/allen_brain_atlas_structures.csv\"))\n",
    "STR_Meta.dropna(inplace=True, subset=[\"atlas_id\"])\n",
    "STR_Meta[\"atlas_id\"] = STR_Meta[\"atlas_id\"].astype(int)\n",
    "STR_Meta = STR_Meta.set_index(\"atlas_id\")\n",
    "\n",
    "# Clean up structure names using modify_str\n",
    "STR_Meta[\"Name2\"] = STR_Meta[\"safe_name\"].apply(modify_str)\n",
    "\n",
    "# Filter to the 213 selected structures\n",
    "Selected_STRs = LoadList(os.path.join(ProjDIR, \"dat/allen-mouse-exp/Structures.txt\"))\n",
    "STR_Meta_2 = STR_Meta[STR_Meta[\"Name2\"].isin(Selected_STRs)].sort_values(\"Name2\")\n",
    "\n",
    "out_path = os.path.join(ProjDIR, \"dat/allen-mouse-exp/Selected_213_STRs.Meta.csv\")\n",
    "STR_Meta_2.to_csv(out_path)\n",
    "print(f\"Selected structures: {len(STR_Meta_2)} (expected 213)\")\n",
    "print(f\"Saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 9. Build Expression Matrix from Raw ISH Files\n",
    "\n",
    "For each human gene with mapped Allen section IDs (from union strategy above),\n",
    "read the raw ISH expression energy from per-section CSV files and compute the\n",
    "**arithmetic mean** across ISH experiments. This produces a (genes × 213\n",
    "structures) matrix of raw expression energy values.\n",
    "\n",
    "This follows Jon's original aggregation method:\n",
    "`colMeans(expression_energy)` across all section datasets per gene.\n",
    "The log2 and QN transforms are applied in notebook 02.\n",
    "\n",
    "See `docs/Jon_Exp_Mat.md` for documentation of the aggregation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "ISH_DIR = config[\"data_files\"][\"ish_expression_dir\"]\n",
    "exp_mat_path = os.path.join(ProjDIR, \"dat/allen-mouse-exp/ExpressionMatrix_raw.parquet\")\n",
    "\n",
    "if os.path.exists(exp_mat_path):\n",
    "    print(f\"Expression matrix already exists: {exp_mat_path}\")\n",
    "    ExpMat = pd.read_parquet(exp_mat_path)\n",
    "    print(f\"  Shape: {ExpMat.shape}\")\n",
    "else:\n",
    "    # Structure mapping: Allen structure 'id' → structure name\n",
    "    structid2name = dict(zip(STR_Meta_2[\"id\"], STR_Meta_2[\"Name2\"]))\n",
    "    selected_struct_ids = set(structid2name.keys())\n",
    "\n",
    "    # Filter gene_sections to only include available ISH files\n",
    "    existing_files = set(os.listdir(ISH_DIR))\n",
    "    gene_sections_available = {}\n",
    "    for hentrez, sids in gene_sections.items():\n",
    "        available = [s for s in sids if f\"{s}.csv\" in existing_files]\n",
    "        if available:\n",
    "            gene_sections_available[hentrez] = available\n",
    "    print(f\"Genes with available ISH data: {len(gene_sections_available)}\")\n",
    "\n",
    "    # Function to process one gene: read ISH CSVs, compute mean expression energy\n",
    "    def process_gene(args, ish_dir, selected_struct_ids, structid2name, structures):\n",
    "        human_entrez, section_ids = args\n",
    "        acc = {s: [] for s in structures}\n",
    "        for sid in section_ids:\n",
    "            csv_path = os.path.join(ish_dir, f\"{sid}.csv\")\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, usecols=[\"structure_id\", \"expression_energy\"])\n",
    "            except (FileNotFoundError, ValueError):\n",
    "                continue\n",
    "            for _, row in df.iterrows():\n",
    "                struct_id = int(row[\"structure_id\"])\n",
    "                if struct_id in selected_struct_ids:\n",
    "                    name = structid2name[struct_id]\n",
    "                    ee = row[\"expression_energy\"]\n",
    "                    if pd.notna(ee):\n",
    "                        acc[name].append(ee)\n",
    "        result = {}\n",
    "        for s in structures:\n",
    "            result[s] = np.mean(acc[s]) if acc[s] else np.nan\n",
    "        return human_entrez, result\n",
    "\n",
    "    structures = sorted(Selected_STRs)\n",
    "    gene_list = list(gene_sections_available.items())\n",
    "\n",
    "    print(f\"Processing {len(gene_list)} genes from {ISH_DIR} ...\")\n",
    "    worker = partial(\n",
    "        process_gene,\n",
    "        ish_dir=ISH_DIR,\n",
    "        selected_struct_ids=selected_struct_ids,\n",
    "        structid2name=structid2name,\n",
    "        structures=structures,\n",
    "    )\n",
    "    with Pool(10) as pool:\n",
    "        results = pool.map(worker, gene_list)\n",
    "\n",
    "    rows = {}\n",
    "    for human_entrez, row_data in results:\n",
    "        rows[human_entrez] = row_data\n",
    "    ExpMat = pd.DataFrame.from_dict(rows, orient=\"index\", columns=structures)\n",
    "    ExpMat.index.name = \"ROW\"\n",
    "    ExpMat = ExpMat.sort_index()\n",
    "\n",
    "    ExpMat.to_parquet(exp_mat_path)\n",
    "    print(f\"Saved: {exp_mat_path}\")\n",
    "    print(f\"  Shape: {ExpMat.shape}\")\n",
    "    print(f\"  NaN fraction: {ExpMat.isna().sum().sum() / ExpMat.size:.4f}\")\n",
    "    print(f\"  Value range: [{np.nanmin(ExpMat.values):.4f}, {np.nanmax(ExpMat.values):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 10. Validate Against Jon's Raw Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "jon_raw_path = os.path.join(ProjDIR, config[\"data_files\"][\"jon_exp_raw\"])\n",
    "if os.path.exists(jon_raw_path):\n",
    "    jon_raw = pd.read_csv(jon_raw_path, index_col=\"ROW\")\n",
    "    common = sorted(set(ExpMat.index) & set(jon_raw.index))\n",
    "    jon_only = sorted(set(jon_raw.index) - set(ExpMat.index))\n",
    "    our_only = sorted(set(ExpMat.index) - set(jon_raw.index))\n",
    "\n",
    "    diff = (ExpMat.loc[common, jon_raw.columns] - jon_raw.loc[common]).abs()\n",
    "    exact = (diff.max(axis=1) < 1e-10).sum()\n",
    "\n",
    "    print(f\"Jon: {len(jon_raw)} genes | Ours: {len(ExpMat)} genes | Common: {len(common)}\")\n",
    "    print(f\"Jon-only: {len(jon_only)} (older HGNC version)\")\n",
    "    print(f\"Ours-only: {len(our_only)}\")\n",
    "    print(f\"Exact match: {exact}/{len(common)} ({100*exact/len(common):.1f}%)\")\n",
    "    print(f\"Max diff: {diff.max().max():.2e}\")\n",
    "else:\n",
    "    print(f\"Jon's raw matrix not found at {jon_raw_path} — skipping validation\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "gencic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ProjDIR = \"/home/jw3514/Work/ASD_Circuits_CellType\"\n",
    "sys.path.insert(1, os.path.join(ProjDIR, \"src\"))\n",
    "from ASD_Circuits import modify_str, LoadList\n",
    "\n",
    "os.chdir(os.path.join(ProjDIR, \"notebooks_mouse_str\"))\n",
    "print(f\"Project root: {ProjDIR}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONN_DIR = os.path.join(ProjDIR, \"dat/allen-mouse-conn\")\n",
    "OUT_DIR = os.path.join(CONN_DIR, \"ConnectomeScoringMat\")\n",
    "ALLEN_DIR = os.path.join(ProjDIR, \"dat/allen-mouse-exp\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Connectome dir:   {CONN_DIR}\")\n",
    "print(f\"Output dir:       {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# 03. Preprocessing Connectivity Data\n",
    "\n",
    "This notebook processes the Allen Mouse Brain Connectivity Atlas data\n",
    "(Oh et al., 2014, *Nature*) into scoring matrices for circuit analysis.\n",
    "\n",
    "**Source data** (supplementary files from the paper):\n",
    "- `nature13186-s4.xlsx` — Connection weights and p-values (ipsilateral & contralateral)\n",
    "- `41586_2014_BFnature13186_MOESM72_ESM.xlsx` — 3D Cartesian distances between structures\n",
    "\n",
    "**Outputs** (saved to `dat/allen-mouse-conn/ConnectomeScoringMat/`):\n",
    "1. `WeightMat.Ipsi.csv` — Ipsilateral weight matrix (213×213, p<0.05 threshold)\n",
    "2. `InfoMat.Ipsi.csv` — Shannon Information scoring matrix (distance-corrected)\n",
    "3. `InfoMat.Ipsi.Short.3900.csv` / `InfoMat.Ipsi.Long.3900.csv` — Short/long range splits\n",
    "4. `Dist_CartesianDistance.csv` — Pairwise distance matrix (213×213)\n",
    "\n",
    "**Methodology** (CCS — Circuit Connectivity Score):\n",
    "- Distance between all 213×213 structure pairs is binned into **10 deciles**\n",
    "- For each bin, connection probability P = (connected pairs) / (total pairs)\n",
    "- Shannon Information:\n",
    "  - Connected pair:   I = −log₂(P)    (rare long-range connections → high information)\n",
    "  - Unconnected pair: I' = −log₂(1−P) (rare absence at short range → high information)\n",
    "- Only **ipsilateral** connections at **p < 0.05** are used (3,063 connections per methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Load Connectivity Data\n",
    "\n",
    "Load raw connectivity weights and p-values from Oh et al. (2014).\n",
    "The data is in acronym format — we convert to full structure names later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ipsilateral and contralateral connectivity from Excel supplementary\n",
    "xlsx_path = os.path.join(CONN_DIR, \"nature13186-s4.xlsx\")\n",
    "W_ipsi = pd.read_excel(xlsx_path, sheet_name=\"W_ipsi\").set_index(\"Unnamed: 0\")\n",
    "PValue_ipsi = pd.read_excel(xlsx_path, sheet_name=\"PValue_ipsi\").set_index(\"Unnamed: 0\")\n",
    "W_contra = pd.read_excel(xlsx_path, sheet_name=\"W_contra\").set_index(\"Unnamed: 0\")\n",
    "PValue_contra = pd.read_excel(xlsx_path, sheet_name=\"PValue_contra\").set_index(\"Unnamed: 0\")\n",
    "\n",
    "print(f\"Raw connectivity matrix: {W_ipsi.shape}\")\n",
    "print(f\"Nonzero ipsi connections (raw): {(W_ipsi > 0).sum().sum()}\")\n",
    "print(f\"Nonzero contra connections (raw): {(W_contra > 0).sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply p-value threshold: zero out connections with p > 0.05\n",
    "P_THRESHOLD = 0.05\n",
    "W_ipsi[PValue_ipsi > P_THRESHOLD] = 0\n",
    "W_contra[PValue_contra > P_THRESHOLD] = 0\n",
    "\n",
    "n_ipsi = int((W_ipsi > 0).sum().sum())\n",
    "n_contra = int((W_contra > 0).sum().sum())\n",
    "print(f\"Ipsi connections after p<{P_THRESHOLD}:   {n_ipsi}\")\n",
    "print(f\"Contra connections after p<{P_THRESHOLD}: {n_contra}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Map Acronyms to Structure Names\n",
    "\n",
    "Convert Allen Atlas acronyms (e.g., \"ACA\") to full structure names\n",
    "(e.g., \"Anterior_cingulate_area\") using the ontology file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology for acronym → name mapping\n",
    "ontology = pd.read_csv(os.path.join(ProjDIR, \"dat/Other/ontology.csv\"))\n",
    "\n",
    "\n",
    "def clean_name(name):\n",
    "    \"\"\"Convert Allen structure name to clean underscore format.\"\"\"\n",
    "    name = re.sub(r'[()]', '', name)\n",
    "    name = re.sub(r'[-]', ' ', name)\n",
    "    return \"_\".join(name.split())\n",
    "\n",
    "\n",
    "acronym2name = {}\n",
    "for _, row in ontology.iterrows():\n",
    "    acronym2name[row[\"acronym\"]] = clean_name(row[\"safe_name\"])\n",
    "\n",
    "# Rename rows and columns from acronyms to full names\n",
    "W_ipsi.columns = [acronym2name[x] for x in W_ipsi.columns]\n",
    "W_ipsi.index = [acronym2name[x] for x in W_ipsi.index]\n",
    "W_contra.columns = [acronym2name[x] for x in W_contra.columns]\n",
    "W_contra.index = [acronym2name[x] for x in W_contra.index]\n",
    "\n",
    "print(f\"Weight matrix: {W_ipsi.shape}\")\n",
    "print(f\"Index sample: {W_ipsi.index[:3].tolist()}\")\n",
    "\n",
    "# Verify structure names match the 213 selected structures from notebook 01\n",
    "Selected_STRs = LoadList(os.path.join(ALLEN_DIR, \"Structures.txt\"))\n",
    "missing = set(Selected_STRs) - set(W_ipsi.index)\n",
    "extra = set(W_ipsi.index) - set(Selected_STRs)\n",
    "print(f\"Structures in connectivity: {len(W_ipsi.index)}\")\n",
    "print(f\"Structures in selected 213: {len(Selected_STRs)}\")\n",
    "if missing:\n",
    "    print(f\"  Missing from connectivity: {missing}\")\n",
    "if extra:\n",
    "    print(f\"  Extra in connectivity: {extra}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Compute Distance Matrices\n",
    "\n",
    "Load 3D Cartesian distances between structures from the supplementary data.\n",
    "We need both ipsilateral and contralateral distances — ipsilateral for\n",
    "the main InfoMat, contralateral for the contra-with-mirror variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_xlsx = os.path.join(CONN_DIR, \"41586_2014_BFnature13186_MOESM72_ESM.xlsx\")\n",
    "RawDistance = pd.read_excel(dist_xlsx, index_col=0)\n",
    "print(f\"Raw distance matrix: {RawDistance.shape}\")\n",
    "\n",
    "# Extract ipsilateral and contralateral distances\n",
    "acronyms = PValue_ipsi.index.values  # original acronym order\n",
    "DistanceMat_ipsi = pd.DataFrame(\n",
    "    data=np.zeros((213, 213)), index=acronyms, columns=acronyms\n",
    ")\n",
    "DistanceMat_contra = pd.DataFrame(\n",
    "    data=np.zeros((213, 213)), index=acronyms, columns=acronyms\n",
    ")\n",
    "for acr_i in acronyms:\n",
    "    for acr_j in acronyms:\n",
    "        DistanceMat_ipsi.loc[acr_i, acr_j] = RawDistance.loc[\n",
    "            f\"{acr_i}_ipsi\", f\"{acr_j}_ipsi\"\n",
    "        ]\n",
    "        DistanceMat_contra.loc[acr_i, acr_j] = RawDistance.loc[\n",
    "            f\"{acr_i}_ipsi\", f\"{acr_j}_contra\"\n",
    "        ]\n",
    "\n",
    "# Rename to full structure names\n",
    "DistanceMat_ipsi.index = W_ipsi.index.values\n",
    "DistanceMat_ipsi.columns = W_ipsi.columns.values\n",
    "DistanceMat_contra.index = W_contra.index.values\n",
    "DistanceMat_contra.columns = W_contra.columns.values\n",
    "\n",
    "print(f\"Ipsi distance range: [{DistanceMat_ipsi.min().min():.1f}, {DistanceMat_ipsi.max().max():.1f}] µm\")\n",
    "print(f\"Contra distance range: [{DistanceMat_contra.min().min():.1f}, {DistanceMat_contra.max().max():.1f}] µm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Compute Shannon Information Scoring Matrix\n",
    "\n",
    "The scoring matrix converts connection status into information-theoretic\n",
    "scores that account for distance-dependent baseline connection probability.\n",
    "\n",
    "**Algorithm**:\n",
    "1. Bin all pairwise ipsi distances into **10 deciles** (equal pair counts)\n",
    "2. For each bin: P = (connected pairs in bin) / (total pairs in bin)\n",
    "3. Shannon Information:\n",
    "   - Connected pair: I = −log₂(P) — connection is informative (especially long range)\n",
    "   - Unconnected pair: I' = −log₂(1−P) — absence is informative (especially short range)\n",
    "4. For each structure pair, assign I or I' based on connection status and distance bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distance decile bins from ALL ipsi pairwise distances\n",
    "ALL_Distances = DistanceMat_ipsi.values.flatten()\n",
    "DistanceDeciles = np.percentile(ALL_Distances, np.arange(0, 100, 10))\n",
    "DistanceDeciles = np.append(DistanceDeciles, max(ALL_Distances))\n",
    "\n",
    "# Count total pairs per distance bin\n",
    "Pairs_per_bin, _ = np.histogram(ALL_Distances, bins=DistanceDeciles)\n",
    "\n",
    "# Count connected pairs per distance bin (using ipsi connections, excluding diagonal/zero distances)\n",
    "conn_distances = DistanceMat_ipsi[(W_ipsi > 0)]\n",
    "conn_distances = conn_distances[~np.isnan(conn_distances)]\n",
    "conn_distances = np.nan_to_num(conn_distances, nan=0).flatten()\n",
    "conn_distances = np.array([x for x in conn_distances if x > 0])\n",
    "\n",
    "Edges_per_bin, _ = np.histogram(conn_distances, bins=DistanceDeciles)\n",
    "\n",
    "# Connection probability and Shannon information per bin\n",
    "P_conn = Edges_per_bin / Pairs_per_bin\n",
    "I_connected = -np.log2(P_conn)          # info for a connected pair\n",
    "I_unconnected = -np.log2(1 - P_conn)    # info for an unconnected pair\n",
    "\n",
    "print(\"Distance bin  | All pairs | Connected | P(conn)  | I_conn  | I_unconn\")\n",
    "print(\"-\" * 75)\n",
    "for i in range(len(DistanceDeciles) - 1):\n",
    "    print(f\"  {DistanceDeciles[i]:7.0f}-{DistanceDeciles[i+1]:7.0f} | {Pairs_per_bin[i]:9d} | \"\n",
    "          f\"{Edges_per_bin[i]:9d} | {P_conn[i]:.5f}  | {I_connected[i]:.3f}   | {I_unconnected[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distance-dependent connection probability and information\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.patch.set_alpha(0)\n",
    "\n",
    "# Connection probability\n",
    "ax1.bar(range(len(Pairs_per_bin)), Pairs_per_bin, color='#b5ffb9', edgecolor='grey',\n",
    "        label='All pairs', alpha=0.7)\n",
    "ax1.bar(range(len(Edges_per_bin)), Edges_per_bin, color='#f9bc86', edgecolor='grey',\n",
    "        label='Connected', alpha=0.8)\n",
    "names = [f\"{DistanceDeciles[i]/1000:.1f}-{DistanceDeciles[i+1]/1000:.1f}\"\n",
    "         for i in range(len(DistanceDeciles) - 1)]\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=45, fontsize=8)\n",
    "ax1.set_xlabel(\"Distance (mm)\")\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.set_title(\"Distance Distribution\")\n",
    "ax1.legend()\n",
    "ax1.patch.set_alpha(0)\n",
    "\n",
    "# Probability and Information\n",
    "x_vals = DistanceDeciles[:-1] / 1000\n",
    "ax2.plot(x_vals, P_conn, 'k-o', label=\"Connection Probability\", markersize=4)\n",
    "ax2.set_xlabel(\"Distance (mm)\")\n",
    "ax2.set_ylabel(\"Connection Probability\", color='black')\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(x_vals, I_connected, 'b-o', label=\"I (connected)\", markersize=4)\n",
    "ax2_twin.plot(x_vals, I_unconnected, 'r-o', label=\"I' (unconnected)\", markersize=4)\n",
    "ax2_twin.set_ylabel(\"Information (bits)\", color='blue')\n",
    "ax2.set_title(\"Distance → Probability → Information\")\n",
    "lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "ax2.patch.set_alpha(0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Information scoring matrix\n",
    "# Each pair gets I_connected or I_unconnected based on ipsi connection status\n",
    "structures = W_ipsi.index.values\n",
    "\n",
    "\n",
    "def assign_info(dist, bins, info_values):\n",
    "    \"\"\"Assign Shannon information value based on which distance bin the pair falls in.\"\"\"\n",
    "    for i in range(len(bins) - 1):\n",
    "        if dist >= bins[i] and dist < bins[i + 1]:\n",
    "            return info_values[i]\n",
    "    return info_values[-1]\n",
    "\n",
    "\n",
    "InfoMat = pd.DataFrame(\n",
    "    data=np.zeros((213, 213)),\n",
    "    index=structures,\n",
    "    columns=structures\n",
    ")\n",
    "\n",
    "for node_i in structures:\n",
    "    for node_j in structures:\n",
    "        if node_i == node_j:\n",
    "            continue\n",
    "        d = DistanceMat_ipsi.loc[node_i, node_j]\n",
    "        w = W_ipsi.loc[node_i, node_j]\n",
    "        if w > 0:\n",
    "            InfoMat.loc[node_i, node_j] = assign_info(d, DistanceDeciles, I_connected)\n",
    "        else:\n",
    "            InfoMat.loc[node_i, node_j] = assign_info(d, DistanceDeciles, I_unconnected)\n",
    "\n",
    "n_nonzero = int((InfoMat > 0).sum().sum())\n",
    "n_expected = 213 * 212  # all non-diagonal pairs\n",
    "print(f\"InfoMat nonzero entries: {n_nonzero} (expected {n_expected})\")\n",
    "print(f\"Info range: [{InfoMat[InfoMat > 0].min().min():.4f}, {InfoMat.max().max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Split into Short-Range and Long-Range\n",
    "\n",
    "Separate pairs by distance using the median connected distance as cutoff.\n",
    "This produces separate scoring matrices for short-range and long-range\n",
    "circuit analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distance cutoff from median of ALL pairwise distances (not just connected)\n",
    "# This gives ~3988 µm, consistent with the v3 reference files named \"3900\"\n",
    "all_nonzero_distances = ALL_Distances[ALL_Distances > 0]\n",
    "DISTANCE_CUTOFF = np.median(all_nonzero_distances)\n",
    "print(f\"Median all-pair distance (cutoff): {DISTANCE_CUTOFF:.1f} µm\")\n",
    "print(f\"Short-range connected: {(conn_distances < DISTANCE_CUTOFF).sum()}\")\n",
    "print(f\"Long-range connected:  {(conn_distances >= DISTANCE_CUTOFF).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split InfoMat and WeightMat by distance\n",
    "InfoMat_short = pd.DataFrame(0.0, index=structures, columns=structures)\n",
    "InfoMat_long = pd.DataFrame(0.0, index=structures, columns=structures)\n",
    "WeightMat_short = pd.DataFrame(0.0, index=structures, columns=structures)\n",
    "WeightMat_long = pd.DataFrame(0.0, index=structures, columns=structures)\n",
    "\n",
    "for node_i in structures:\n",
    "    for node_j in structures:\n",
    "        if node_i == node_j:\n",
    "            continue\n",
    "        d = DistanceMat_ipsi.loc[node_i, node_j]\n",
    "        info = InfoMat.loc[node_i, node_j]\n",
    "        w = W_ipsi.loc[node_i, node_j]\n",
    "        if d < DISTANCE_CUTOFF:\n",
    "            InfoMat_short.loc[node_i, node_j] = info\n",
    "            if w > 0:\n",
    "                WeightMat_short.loc[node_i, node_j] = w\n",
    "        else:\n",
    "            InfoMat_long.loc[node_i, node_j] = info\n",
    "            if w > 0:\n",
    "                WeightMat_long.loc[node_i, node_j] = w\n",
    "\n",
    "print(f\"Short-range: {int((WeightMat_short > 0).sum().sum())} connections, \"\n",
    "      f\"{int((InfoMat_short > 0).sum().sum())} info entries\")\n",
    "print(f\"Long-range:  {int((WeightMat_long > 0).sum().sum())} connections, \"\n",
    "      f\"{int((InfoMat_long > 0).sum().sum())} info entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 6. Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all matrices\n",
    "W_ipsi.to_csv(os.path.join(OUT_DIR, \"WeightMat.Ipsi.csv\"))\n",
    "InfoMat.to_csv(os.path.join(OUT_DIR, \"InfoMat.Ipsi.csv\"))\n",
    "InfoMat_short.to_csv(os.path.join(OUT_DIR, \"InfoMat.Ipsi.Short.3900.csv\"))\n",
    "InfoMat_long.to_csv(os.path.join(OUT_DIR, \"InfoMat.Ipsi.Long.3900.csv\"))\n",
    "WeightMat_short.to_csv(os.path.join(OUT_DIR, \"WeightMat.Ipsi.Short.3900.csv\"))\n",
    "WeightMat_long.to_csv(os.path.join(OUT_DIR, \"WeightMat.Ipsi.Long.3900.csv\"))\n",
    "DistanceMat_ipsi.to_csv(os.path.join(CONN_DIR, \"Dist_CartesianDistance.csv\"))\n",
    "\n",
    "# Also save lowercase aliases for backward compatibility\n",
    "InfoMat_short.to_csv(os.path.join(OUT_DIR, \"InfoMat.Ipsi.short.csv\"))\n",
    "InfoMat_long.to_csv(os.path.join(OUT_DIR, \"InfoMat.Ipsi.long.csv\"))\n",
    "\n",
    "print(\"Saved:\")\n",
    "for f in [\"WeightMat.Ipsi.csv\", \"InfoMat.Ipsi.csv\",\n",
    "          \"InfoMat.Ipsi.Short.3900.csv\", \"InfoMat.Ipsi.Long.3900.csv\",\n",
    "          \"WeightMat.Ipsi.Short.3900.csv\", \"WeightMat.Ipsi.Long.3900.csv\"]:\n",
    "    print(f\"  {OUT_DIR}/{f}\")\n",
    "print(f\"  {CONN_DIR}/Dist_CartesianDistance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 7. Validate Against Reference (v3)\n",
    "\n",
    "Compare newly generated matrices against the original v3 files\n",
    "(from `ScoreingMat_jw_v3/` in the old project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v3 reference files\n",
    "ref_weight = pd.read_csv(os.path.join(OUT_DIR, \"WeightMat.Ipsi.csv.v3ref\"), index_col=0)\n",
    "ref_info = pd.read_csv(os.path.join(OUT_DIR, \"InfoMat.Ipsi.csv.v3ref\"), index_col=0)\n",
    "\n",
    "# Align indices (should be identical)\n",
    "common_strs = ref_weight.index.intersection(W_ipsi.index)\n",
    "print(f\"Common structures: {len(common_strs)}\")\n",
    "\n",
    "# Compare WeightMat\n",
    "w_new = W_ipsi.loc[common_strs, common_strs].values.flatten()\n",
    "w_ref = ref_weight.loc[common_strs, common_strs].values.flatten()\n",
    "from scipy.stats import pearsonr\n",
    "r_w, _ = pearsonr(w_new, w_ref)\n",
    "n_match_w = np.sum(w_new == w_ref)\n",
    "print(f\"\\nWeightMat comparison:\")\n",
    "print(f\"  Pearson r:   {r_w:.6f}\")\n",
    "print(f\"  Exact match: {n_match_w}/{len(w_new)} ({100*n_match_w/len(w_new):.1f}%)\")\n",
    "print(f\"  New nonzero: {int(np.sum(w_new > 0))}\")\n",
    "print(f\"  Ref nonzero: {int(np.sum(w_ref > 0))}\")\n",
    "\n",
    "# Compare InfoMat\n",
    "i_new = InfoMat.loc[common_strs, common_strs].values.flatten()\n",
    "i_ref = ref_info.loc[common_strs, common_strs].values.flatten()\n",
    "r_i, _ = pearsonr(i_new, i_ref)\n",
    "mae_i = np.mean(np.abs(i_new - i_ref))\n",
    "n_match_i = np.sum(np.isclose(i_new, i_ref, atol=1e-10))\n",
    "print(f\"\\nInfoMat comparison:\")\n",
    "print(f\"  Pearson r:   {r_i:.6f}\")\n",
    "print(f\"  MAE:         {mae_i:.6f}\")\n",
    "print(f\"  Close match: {n_match_i}/{len(i_new)} ({100*n_match_i/len(i_new):.1f}%)\")\n",
    "\n",
    "# Compare short/long InfoMat\n",
    "ref_short = pd.read_csv(os.path.join(OUT_DIR, \"InfoMat.Ipsi.Short.3900.csv.v3ref\"), index_col=0)\n",
    "ref_long = pd.read_csv(os.path.join(OUT_DIR, \"InfoMat.Ipsi.Long.3900.csv.v3ref\"), index_col=0)\n",
    "s_new = InfoMat_short.loc[common_strs, common_strs].values.flatten()\n",
    "s_ref = ref_short.loc[common_strs, common_strs].values.flatten()\n",
    "l_new = InfoMat_long.loc[common_strs, common_strs].values.flatten()\n",
    "l_ref = ref_long.loc[common_strs, common_strs].values.flatten()\n",
    "r_s, _ = pearsonr(s_new[s_new + s_ref > 0], s_ref[s_new + s_ref > 0])\n",
    "r_l, _ = pearsonr(l_new[l_new + l_ref > 0], l_ref[l_new + l_ref > 0])\n",
    "print(f\"\\nInfoMat Short r: {r_s:.6f}\")\n",
    "print(f\"InfoMat Long r:  {r_l:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Output | Shape | Path |\n",
    "|--------|-------|------|\n",
    "| Weight Matrix (ipsi, p<0.05) | 213 × 213 | `ConnectomeScoringMat/WeightMat.Ipsi.csv` |\n",
    "| Info Matrix (ipsi) | 213 × 213 | `ConnectomeScoringMat/InfoMat.Ipsi.csv` |\n",
    "| Info Matrix (short, <3900µm) | 213 × 213 | `ConnectomeScoringMat/InfoMat.Ipsi.Short.3900.csv` |\n",
    "| Info Matrix (long, ≥3900µm) | 213 × 213 | `ConnectomeScoringMat/InfoMat.Ipsi.Long.3900.csv` |\n",
    "| Distance Matrix | 213 × 213 | `dat/allen-mouse-conn/Dist_CartesianDistance.csv` |\n",
    "\n",
    "**Source**: Oh et al., \"A mesoscale connectome of the mouse brain\", *Nature* 508, 207–214 (2014)\n",
    "\n",
    "**CCS Definition**: CCS(S) = Σ I(i,j) / n_pairs for structures i,j in circuit S,\n",
    "where I is Shannon information based on connection probability conditioned on distance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "gencic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

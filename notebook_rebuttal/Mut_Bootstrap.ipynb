{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "ProjDIR = \"/home/jw3514/Work/ASD_Circuits_CellType/\" # Change to your project directory\n",
    "sys.path.insert(1, f'{ProjDIR}/src/')\n",
    "from ASD_Circuits import *\n",
    "\n",
    "try:\n",
    "    os.chdir(f\"{ProjDIR}/notebook_rebuttal/\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not change directory - {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "HGNC, ENSID2Entrez, GeneSymbol2Entrez, Entrez2Symbol = LoadGeneINFO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "with open(\"../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "expr_matrix_path = config[\"analysis_types\"][\"STR_ISH\"][\"expr_matrix\"]\n",
    "STR_BiasMat = pd.read_parquet(f\"../{expr_matrix_path}\")\n",
    "Anno = STR2Region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/home/jw3514/Work/data/DDD/41586_2020_2832_MOESM4_ESM.xlsx\")\n",
    "df = df.sort_values(\"denovoWEST_p_full\")\n",
    "hc_df = df[df[\"denovoWEST_p_full\"]<=0.05/18762]\n",
    "entrez_ids = [int(GeneSymbol2Entrez.get(x, -1)) for x in hc_df[\"symbol\"].values]\n",
    "hc_df[\"EntrezID\"] = entrez_ids\n",
    "hc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df[\"AutismMerged_LoF\"] = (\n",
    "    df.loc[hc_df.index, \"frameshift_variant\"].fillna(0)\n",
    "    + df.loc[hc_df.index, \"splice_acceptor_variant\"].fillna(0)\n",
    "    + df.loc[hc_df.index, \"splice_donor_variant\"].fillna(0)\n",
    "    + df.loc[hc_df.index, \"stop_gained\"].fillna(0)\n",
    "    + df.loc[hc_df.index, \"stop_lost\"].fillna(0)\n",
    ").astype(int).clip(lower=0)\n",
    "\n",
    "hc_df[\"AutismMerged_Dmis_REVEL0.5\"] = df.loc[hc_df.index, \"missense_variant\"].fillna(0).astype(int).clip(lower=0)\n",
    "\n",
    "hc_df = hc_df[[\"EntrezID\", \"symbol\", \"AutismMerged_LoF\", \"AutismMerged_Dmis_REVEL0.5\"]]\n",
    "hc_df = hc_df.set_index(\"EntrezID\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude ASD genes from hc_df before bootstrap\n",
    "ASD_GW = Fil2Dict(ProjDIR+\"dat/Genetics/GeneWeights_DN/Spark_Meta_EWS.GeneWeight.DN.gw\")\n",
    "ASD_GENES = list(ASD_GW.keys())\n",
    "print(f\"Total genes in hc_df before excluding ASD: {len(hc_df)}\")\n",
    "print(f\"Number of ASD genes to exclude: {len(ASD_GENES)}\")\n",
    "\n",
    "# Filter out ASD genes\n",
    "hc_df = hc_df[~hc_df[\"EntrezID\"].isin(ASD_GENES)]\n",
    "print(f\"Total genes in hc_df after excluding ASD: {len(hc_df)}\")\n",
    "print(f\"Excluded {len(ASD_GENES)} ASD genes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDD_hc_GW = Aggregate_Gene_Weights_NDD(hc_df, out=\"../dat/GeneWeights/DDD.hc.gw.csv\")\n",
    "# NDD_top61_DF = hc_df.head(61)\n",
    "# DDD_top61_GW = Aggregate_Gene_Weights_NDD(NDD_top61_DF, out=\"../dat/GeneWeights/DDD.top61.gw.csv\")\n",
    "#Dict2Fil(DDD_top61_GW, \"../dat/GeneWeights/DDD.top61.gw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_gene_mutations(\n",
    "    df,\n",
    "    n_boot=10,\n",
    "    weighted=True,\n",
    "    lof_col=\"AutismMerged_LoF\",\n",
    "    dmis_col=\"AutismMerged_Dmis_REVEL0.5\",\n",
    "    rng=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Bootstrap mutation counts at the mutation level, preserving gene identity\n",
    "    and total mutation load. Supports weighted (mutation-rate) or uniform resampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with gene-level LOF and Dmis mutation counts.\n",
    "    n_boot : int, optional\n",
    "        Number of bootstrap replicates (default = 10).\n",
    "    weighted : bool, optional\n",
    "        If True, mutations are resampled with probability proportional to observed counts.\n",
    "        If False, each gene has equal probability of receiving any mutation.\n",
    "    lof_col, dmis_col : str\n",
    "        Column names for LOF and Dmis counts.\n",
    "    rng : np.random.Generator, optional\n",
    "        Numpy random generator for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    boot_DFs : list of pd.DataFrame\n",
    "        List of bootstrapped dataframes with resampled mutation counts and 'bootstrap_iter' column.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "    n = len(df)\n",
    "    boot_DFs = []\n",
    "\n",
    "    # Ensure integer non-negative mutation counts\n",
    "    # lof = (df[\"frameshift_variant\"] + df[\"splice_acceptor_variant\"] + df[\"splice_donor_variant\"] + df[\"stop_gained\"] + df[\"stop_lost\"]).astype(int).clip(lower=0)\n",
    "    # dmis = df[\"missense_variant\"].astype(int).clip(lower=0) \n",
    "\n",
    "    lof = df[lof_col].astype(int).clip(lower=0)\n",
    "    dmis = df[dmis_col].astype(int).clip(lower=0)\n",
    "\n",
    "    total_lof = lof.sum()\n",
    "    total_dmis = dmis.sum()\n",
    "\n",
    "    # Probability vectors for mutation assignment\n",
    "    if weighted:\n",
    "        # Weighted by observed mutation burden per gene\n",
    "        p_lof = lof / total_lof if total_lof > 0 else np.ones(n) / n\n",
    "        p_dmis = dmis / total_dmis if total_dmis > 0 else np.ones(n) / n\n",
    "    else:\n",
    "        # Uniform: every gene equally likely\n",
    "        p_lof = np.ones(n) / n\n",
    "        p_dmis = np.ones(n) / n\n",
    "\n",
    "    for i in range(1, n_boot + 1):\n",
    "        # Draw total_lof mutation events, assign to genes\n",
    "        new_lof_counts = np.bincount(\n",
    "            rng.choice(n, size=total_lof, replace=True, p=p_lof),\n",
    "            minlength=n\n",
    "        )\n",
    "        new_dmis_counts = np.bincount(\n",
    "            rng.choice(n, size=total_dmis, replace=True, p=p_dmis),\n",
    "            minlength=n\n",
    "        )\n",
    "\n",
    "        # Create bootstrap replicate\n",
    "        df_boot = df.copy().reset_index(drop=True)\n",
    "        df_boot[lof_col] = new_lof_counts\n",
    "        df_boot[dmis_col] = new_dmis_counts\n",
    "        df_boot[\"bootstrap_iter\"] = i\n",
    "        df_boot[\"bootstrap_type\"] = \"weighted\" if weighted else \"uniform\"\n",
    "        boot_DFs.append(df_boot)\n",
    "\n",
    "    return boot_DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_DFs_weights = bootstrap_gene_mutations(hc_df, 1000, weighted=True, lof_col=\"AutismMerged_LoF\", dmis_col=\"AutismMerged_Dmis_REVEL0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_DFs_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Aggregate_Gene_Weights_NDD2(MutFil, usepLI=False, Bmis=False, out=None):\n",
    "    gene2MutN = {}\n",
    "    for i, row in MutFil.iterrows():\n",
    "        try:\n",
    "            g = int(row[\"EntrezID\"])\n",
    "        except:\n",
    "            print(g, \"Error converting Entrez ID\")\n",
    "\n",
    "        nLGD = row[\"AutismMerged_LoF\"] \n",
    "        nMis = row[\"AutismMerged_Dmis_REVEL0.5\"] \n",
    "\n",
    "        gene2MutN[g] = nLGD * 0.347 + nMis * 0.194\n",
    "    if out != None:\n",
    "        writer = csv.writer(open(out, 'wt'))\n",
    "        for k,v in sorted(gene2MutN.items(), key=lambda x:x[1], reverse=True):\n",
    "           writer.writerow([k,v]) \n",
    "    return gene2MutN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "\n",
    "def process_bootstrap_iter(args):\n",
    "    \"\"\"Worker function to process a single bootstrap iteration\"\"\"\n",
    "    i, DF, save_dir, str_bias_mat = args\n",
    "    boot_gw = Aggregate_Gene_Weights_NDD2(DF)\n",
    "    boot_bias = MouseSTR_AvgZ_Weighted(str_bias_mat, boot_gw)\n",
    "    boot_bias.to_csv(os.path.join(save_dir, f\"DDD_ExomeWide.GeneWeight.boot{i}.csv\"))\n",
    "    return i, boot_bias\n",
    "\n",
    "save_dir = \"../results/Bootstrap_bias/DDD_ExomeWide/Weighted_Resampling\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Prepare arguments for parallel processing\n",
    "n_workers = mp.cpu_count()  # Use all available CPU cores\n",
    "args_list = [(i, DF, save_dir, STR_BiasMat) for i, DF in enumerate(boot_DFs_weights)]\n",
    "\n",
    "# Process in parallel\n",
    "boot_bias_list_weights = [None] * len(boot_DFs_weights)  # Pre-allocate list to maintain order\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_idx = {executor.submit(process_bootstrap_iter, args): args[0] for args in args_list}\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(future_to_idx):\n",
    "        i, boot_bias = future.result()\n",
    "        boot_bias_list_weights[i] = boot_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstraped bias residue\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fit_structure_bias_linear_model(merged_data, metric='EFFECT', suffixes=('_1', '_2')):\n",
    "    \"\"\"Fit linear regression model and compute residuals\"\"\"\n",
    "    X = merged_data[f'{metric}{suffixes[1]}'].values.reshape(-1, 1)\n",
    "    y = merged_data[f'{metric}{suffixes[0]}'].values\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "\n",
    "    results_df = merged_data.copy()\n",
    "    results_df['predicted'] = y_pred\n",
    "    results_df['residual'] = residuals\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def merge_str_bias_datasets(dataset1, dataset2, suffixes=('_1', '_2')):\n",
    "    \"\"\"\n",
    "    Merge two structure bias datasets for comparison and compute residuals.\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    dataset1_cols = ['Rank', 'EFFECT', 'Region'] if 'Region' in dataset1.columns else ['Rank', 'EFFECT']\n",
    "    dataset2_cols = ['Rank', 'EFFECT']\n",
    "    \n",
    "    # Merge the datasets on structure names\n",
    "    merged_data = pd.merge(dataset1[dataset1_cols], dataset2[dataset2_cols], \n",
    "                          left_index=True, right_index=True, suffixes=suffixes)\n",
    "\n",
    "    # Calculate differences\n",
    "    merged_data[f'DIFF_Rank'] = merged_data[f'Rank{suffixes[0]}'] - merged_data[f'Rank{suffixes[1]}']\n",
    "    merged_data[f'ABS_DIFF_Rank'] = np.abs(merged_data[f'DIFF_Rank'])\n",
    "    \n",
    "    merged_data[f'DIFF_EFFECT'] = merged_data[f'EFFECT{suffixes[0]}'] - merged_data[f'EFFECT{suffixes[1]}']\n",
    "    merged_data[f'ABS_DIFF_EFFECT'] = np.abs(merged_data[f'DIFF_EFFECT'])\n",
    "\n",
    "    # Sort by absolute difference in EFFECT\n",
    "    merged_data = merged_data.sort_values('ABS_DIFF_EFFECT', ascending=False)\n",
    "    \n",
    "    # Fit linear model and compute residuals\n",
    "    merged_data = fit_structure_bias_linear_model(merged_data, metric='EFFECT', suffixes=suffixes)\n",
    "    \n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load observed ASD bias\n",
    "Spark_ASD_STR_Bias = pd.read_csv(\"../dat/Unionize_bias/Spark_Meta_EWS.Z2.bias.FDR.csv\", index_col=0)\n",
    "Spark_ASD_STR_Bias[\"Region\"] = Spark_ASD_STR_Bias[\"REGION\"]\n",
    "\n",
    "# Load bootstrap results (if already computed, otherwise use boot_bias_list_weights)\n",
    "ASD_DIR = \"../results/Bootstrap_bias/Spark_ExomeWide/Weighted_Resampling\"\n",
    "DDD_DIR = \"../results/Bootstrap_bias/DDD_ExomeWide/Weighted_Resampling\"\n",
    "\n",
    "# Use boot_bias_list_weights if available, otherwise load from files\n",
    "if 'boot_bias_list_weights' in locals() and len(boot_bias_list_weights) > 0:\n",
    "    DDD_Boots = boot_bias_list_weights\n",
    "else:\n",
    "    DDD_Boots = [pd.read_csv(f\"{DDD_DIR}/DDD_ExomeWide.GeneWeight.boot{i}.csv\", index_col=0) for i in range(1000)]\n",
    "\n",
    "# Load ASD boots if available\n",
    "if os.path.exists(ASD_DIR):\n",
    "    ASD_Boots = [pd.read_csv(f\"{ASD_DIR}/Spark_ExomeWide.GeneWeight.boot{i}.csv\", index_col=0) for i in range(1000)]\n",
    "else:\n",
    "    print(f\"Warning: ASD bootstrap directory not found at {ASD_DIR}\")\n",
    "    ASD_Boots = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residues for each bootstrap iteration (parallelized)\n",
    "print(\"Computing residues for each bootstrap iteration...\")\n",
    "\n",
    "def compute_residuals_for_boot(args):\n",
    "    \"\"\"Worker function to compute residuals for a single bootstrap iteration\"\"\"\n",
    "    i, ddd_boot, asd_bias = args\n",
    "    merged = merge_str_bias_datasets(asd_bias, ddd_boot, suffixes=('_ASD', '_DD'))\n",
    "    # Return index and residuals as a dictionary\n",
    "    return i, merged['residual'].to_dict()\n",
    "\n",
    "# Prepare arguments for parallel processing\n",
    "n_workers = mp.cpu_count()\n",
    "args_list = [(i, ddd_boot, Spark_ASD_STR_Bias) for i, ddd_boot in enumerate(DDD_Boots)]\n",
    "\n",
    "# Process in parallel\n",
    "print(f\"Using {n_workers} workers to compute residues...\")\n",
    "residual_results = [None] * len(DDD_Boots)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    future_to_idx = {executor.submit(compute_residuals_for_boot, args): args[0] for args in args_list}\n",
    "    \n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_idx):\n",
    "        i, residuals_dict = future.result()\n",
    "        residual_results[i] = residuals_dict\n",
    "        completed += 1\n",
    "        if completed % 100 == 0:\n",
    "            print(f\"Completed {completed}/{len(DDD_Boots)} bootstrap iterations\")\n",
    "\n",
    "# Collect all residues for each structure\n",
    "structure_residuals = {}  # {structure_name: [residual1, residual2, ...]}\n",
    "for residuals_dict in residual_results:\n",
    "    for structure, residual in residuals_dict.items():\n",
    "        if structure not in structure_residuals:\n",
    "            structure_residuals[structure] = []\n",
    "        structure_residuals[structure].append(residual)\n",
    "\n",
    "print(f\"Computed residues for {len(structure_residuals)} structures across {len(DDD_Boots)} bootstrap iterations\")\n",
    "\n",
    "# Calculate 95% CI for each structure\n",
    "residual_ci = {}\n",
    "for structure, residuals in structure_residuals.items():\n",
    "    residuals_array = np.array(residuals)\n",
    "    ci_lower = np.percentile(residuals_array, 2.5)\n",
    "    ci_upper = np.percentile(residuals_array, 97.5)\n",
    "    median_residual = np.median(residuals_array)\n",
    "    mean_residual = np.mean(residuals_array)\n",
    "    \n",
    "    residual_ci[structure] = {\n",
    "        'mean': mean_residual,\n",
    "        'median': median_residual,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'std': np.std(residuals_array)\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easier viewing\n",
    "residual_ci_df = pd.DataFrame(residual_ci).T\n",
    "residual_ci_df = residual_ci_df.sort_values('median', ascending=False)\n",
    "residual_ci_df.index.name = 'Structure'\n",
    "\n",
    "print(\"\\n95% CI of bias residues computed for all structures\")\n",
    "print(f\"\\nTop 10 structures by median residual:\")\n",
    "print(residual_ci_df.head(10))\n",
    "\n",
    "# Save results\n",
    "save_dir_ci = \"../results/Bootstrap_bias/DDD_ExomeWide/Residual_CI\"\n",
    "os.makedirs(save_dir_ci, exist_ok=True)\n",
    "residual_ci_df.to_csv(os.path.join(save_dir_ci, \"DDD_ExomeWide.Residual_CI_95.csv\"))\n",
    "print(f\"\\nResults saved to: {save_dir_ci}/DDD_ExomeWide.Residual_CI_95.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Circuit_STRs if available\n",
    "try:\n",
    "    GENCIC = pd.read_csv('../results/GENCIC_MouseSTRBias.csv', index_col=0)\n",
    "    Circuit_STRs = GENCIC[GENCIC[\"Circuits.46\"]==1][\"Structure\"].values\n",
    "except:\n",
    "    print(\"Warning: Could not load Circuit_STRs, using all structures\")\n",
    "    Circuit_STRs = None\n",
    "\n",
    "# Compute merged_data2 from observed ASD vs DDD (exclude ASD)\n",
    "DDD_GW = Fil2Dict(config[\"gene_sets\"][\"DDD_293\"][\"geneweights\"])\n",
    "DDD_STR_Bias = MouseSTR_AvgZ_Weighted(STR_BiasMat, DDD_GW)\n",
    "DDD_STR_Bias[\"Region\"] = [Anno.get(ct_idx, \"Unknown\") for ct_idx in DDD_STR_Bias.index.values]\n",
    "\n",
    "# Exclude ASD genes from DDD\n",
    "ASD_GW = Fil2Dict(ProjDIR+\"dat/Genetics/GeneWeights_DN/Spark_Meta_EWS.GeneWeight.DN.gw\")\n",
    "ASD_GENES = list(ASD_GW.keys())\n",
    "DDD_GW_filt_ASD = {k: v for k, v in DDD_GW.items() if k not in ASD_GENES}\n",
    "DDD_rmASD_STR_Bias = MouseSTR_AvgZ_Weighted(STR_BiasMat, DDD_GW_filt_ASD)\n",
    "DDD_rmASD_STR_Bias[\"Region\"] = [Anno.get(ct_idx, \"Unknown\") for ct_idx in DDD_rmASD_STR_Bias.index.values]\n",
    "\n",
    "# Merge observed data\n",
    "merged_data2 = merge_str_bias_datasets(Spark_ASD_STR_Bias, DDD_rmASD_STR_Bias, suffixes=('_ASD', '_DD_ExcludeASD'))\n",
    "\n",
    "# Filter to circuit structures if available\n",
    "if Circuit_STRs is not None:\n",
    "    merged_data_eval = merged_data2[merged_data2.index.isin(Circuit_STRs)]\n",
    "else:\n",
    "    merged_data_eval = merged_data2.copy()\n",
    "\n",
    "def plot_top_residual_structures_with_CI(merged_data, residual_ci_df, top_n=30, top_threshold=40, \n",
    "                                         name1=\"ASD\", name2=\"DD\", figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot brain structures with largest residuals from regression analysis, including 95% CI error bars.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    merged_data : DataFrame\n",
    "        Merged dataset with residual and region information\n",
    "    residual_ci_df : DataFrame\n",
    "        DataFrame with CI information (columns: ci_lower, ci_upper, median, mean)\n",
    "    top_n : int\n",
    "        Number of top structures to display\n",
    "    top_threshold : int\n",
    "        Filter to structures in top N of at least one dataset\n",
    "    name1, name2 : str\n",
    "        Names of the two datasets being compared\n",
    "    figsize : tuple\n",
    "        Figure size (width, height)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    top_diff : DataFrame\n",
    "        Top structures with largest residuals\n",
    "    \"\"\"\n",
    "    # Filter to only structures that appear in top threshold of at least one dataset\n",
    "    top_structures = merged_data[(merged_data[f\"Rank_{name1}\"] <= top_threshold) | \n",
    "                                (merged_data[f\"Rank_{name2}\"] <= top_threshold)]\n",
    "\n",
    "    print(f\"Total structures in top {top_threshold} of at least one dataset: {len(top_structures)}\")\n",
    "\n",
    "    # Sort by absolute difference for top structures only\n",
    "    top_structures = top_structures.copy()\n",
    "    top_structures[\"ABS_DIFF\"] = abs(merged_data[f\"residual\"])\n",
    "    top_structures = top_structures.sort_values('ABS_DIFF', ascending=True)\n",
    "\n",
    "    # Take the top N structures with largest differences from those in top threshold\n",
    "    top_n = min(top_n, len(top_structures))\n",
    "    top_diff = top_structures.tail(top_n)\n",
    "\n",
    "    print(f\"Showing top {len(top_diff)} structures with largest differences (from top {top_threshold} filter)\")\n",
    "\n",
    "    # Merge with CI data\n",
    "    top_diff = top_diff.merge(residual_ci_df[['ci_lower', 'ci_upper', 'median']], \n",
    "                             left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    # Use observed residual (real data) for bar height, CI for error bars\n",
    "    top_diff['residual_plot'] = top_diff['residual']  # Use observed residual\n",
    "    top_diff['ci_lower_plot'] = top_diff['ci_lower'].fillna(top_diff['residual'])\n",
    "    top_diff['ci_upper_plot'] = top_diff['ci_upper'].fillna(top_diff['residual'])\n",
    "\n",
    "    # Define regions and colors\n",
    "    REGIONS_seq = ['Isocortex','Olfactory_areas', 'Cortical_subplate', \n",
    "                    'Hippocampus','Amygdala','Striatum', \n",
    "                    \"Thalamus\", \"Hypothalamus\", \"Midbrain\", \n",
    "                    \"Medulla\", \"Pallidum\", \"Pons\", \n",
    "                    \"Cerebellum\"]\n",
    "    REG_COR_Dic = dict(zip(REGIONS_seq, [\"#268ad5\", \"#D5DBDB\", \"#7ac3fa\", \n",
    "                                        \"#2c9d39\", \"#742eb5\", \"#ed8921\", \n",
    "                                        \"#e82315\", \"#E6B0AA\", \"#f6b26b\",  \n",
    "                                        \"#20124d\", \"#2ECC71\", \"#D2B4DE\", \n",
    "                                        \"#ffd966\", ]))\n",
    "\n",
    "    # Create publication-quality plot of residuals for top_diff structures\n",
    "    plt.rcParams.update({'font.size': 12, 'font.family': 'Arial'})\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=300)\n",
    "\n",
    "    # Sort top_diff by residuals for better visualization\n",
    "    top_diff_sorted = top_diff.sort_values('ABS_DIFF', ascending=True)\n",
    "\n",
    "    # Create colors based on region\n",
    "    colors = [REG_COR_Dic.get(region, '#808080') for region in top_diff_sorted['Region']]\n",
    "\n",
    "    # Calculate error bar positions\n",
    "    y_pos = range(len(top_diff_sorted))\n",
    "    x_vals = top_diff_sorted['residual_plot'].values\n",
    "    xerr_lower = x_vals - top_diff_sorted['ci_lower_plot'].values\n",
    "    xerr_upper = top_diff_sorted['ci_upper_plot'].values - x_vals\n",
    "    \n",
    "    # Create horizontal bar plot with error bars\n",
    "    bars = ax.barh(y_pos, x_vals, \n",
    "                   color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add error bars\n",
    "    ax.errorbar(x_vals, y_pos, \n",
    "                xerr=[xerr_lower, xerr_upper],\n",
    "                fmt='none', ecolor='black', elinewidth=1.5, capsize=3, capthick=1.5, alpha=0.7)\n",
    "\n",
    "    # Customize the plot with publication-quality styling\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels([name.replace('_', ' ') for name in top_diff_sorted.index], \n",
    "                       fontsize=15, fontweight='normal')\n",
    "    ax.set_xlabel(f'Residuals ({name1} vs {name2}) with 95% CI', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Remove top and right spines for cleaner look\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.spines['bottom'].set_linewidth(1)\n",
    "\n",
    "    # Add subtle grid\n",
    "    ax.grid(True, axis='x', alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # Add vertical line at x=0 with better styling\n",
    "    ax.axvline(x=0, color='black', linestyle='-', alpha=0.7, linewidth=1)\n",
    "\n",
    "    # Create legend for regions with better styling\n",
    "    unique_regions = sorted(list(set(top_diff_sorted['Region'])))\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=REG_COR_Dic.get(region, '#808080'), \n",
    "                                    alpha=0.8, edgecolor='black', linewidth=0.5) \n",
    "                       for region in unique_regions if region in REG_COR_Dic]\n",
    "    legend_labels = [region.replace('_', ' ') for region in unique_regions if region in REG_COR_Dic]\n",
    "\n",
    "    if legend_elements:\n",
    "        ax.legend(\n",
    "            legend_elements, legend_labels,\n",
    "            loc='center left',\n",
    "            bbox_to_anchor=(0.80, 0.15),\n",
    "            fontsize=10, \n",
    "            frameon=True,\n",
    "            fancybox=True,\n",
    "            shadow=True,\n",
    "            framealpha=0.9\n",
    "        )\n",
    "\n",
    "    # Adjust layout and margins\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0.3)  # Make room for structure names\n",
    "    plt.show()\n",
    "    \n",
    "    return top_diff\n",
    "\n",
    "# Create the plot with 95% CI\n",
    "top_diff = plot_top_residual_structures_with_CI(merged_data_eval, residual_ci_df, \n",
    "                                                top_n=20, top_threshold=40,\n",
    "                                                name1=\"ASD\", name2=\"DD_ExcludeASD\", \n",
    "                                                figsize=(6, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOEUF25 Bootstrap Analysis\n",
    "\n",
    "# Load gnomad4 constraint data and define LOEUF25 gene set\n",
    "gnomad4 = pd.read_csv(\"/home/jw3514/Work/data/gnomad/gnomad.v4.0.constraint_metrics.tsv\", sep=\"\\t\")\n",
    "search_text = 'ENST'\n",
    "gnomad4 = gnomad4[(gnomad4[\"transcript\"].str.contains(search_text))]\n",
    "gnomad4 = gnomad4[gnomad4[\"mane_select\"]==True]\n",
    "\n",
    "# Convert gene symbols to Entrez IDs\n",
    "for i, row in gnomad4.iterrows():\n",
    "    symbol = row[\"gene\"]\n",
    "    gnomad4.loc[i, \"Entrez\"] = int(GeneSymbol2Entrez.get(symbol, 0))\n",
    "\n",
    "# Take subset where lof.oe_ci.upper is in the bottom 25% (most constrained)\n",
    "bottom_25_percent_threshold = gnomad4[\"lof.oe_ci.upper\"].quantile(0.25)\n",
    "gnomad4_bottom25 = gnomad4[gnomad4[\"lof.oe_ci.upper\"] <= bottom_25_percent_threshold]\n",
    "columns_to_keep_g4 = [\"Entrez\", \"gene\", \"lof.pLI\", \"lof.z_score\", \"lof.oe_ci.upper\"]\n",
    "gnomad4_bottom25 = gnomad4_bottom25[columns_to_keep_g4]\n",
    "gnomad4_bottom25 = gnomad4_bottom25.sort_values(by=\"lof.oe_ci.upper\", ascending=True)\n",
    "\n",
    "# Make sure Entrez is int and exclude rows with Entrez = 0\n",
    "gnomad4_bottom25[\"Entrez\"] = gnomad4_bottom25[\"Entrez\"].astype(int)\n",
    "gnomad4_bottom25 = gnomad4_bottom25[gnomad4_bottom25[\"Entrez\"] != 0]\n",
    "\n",
    "# Get LOEUF25 gene list (Entrez IDs)\n",
    "LOEUF25_genes = gnomad4_bottom25[\"Entrez\"].unique().tolist()\n",
    "print(f\"LOEUF25 gene set: {len(LOEUF25_genes)} genes\")\n",
    "print(f\"Bottom 25% threshold: {bottom_25_percent_threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_genes_from_set(gene_set, n_boot=1000, n_genes=None, rng=None):\n",
    "    \"\"\"\n",
    "    Bootstrap sample genes from a given gene set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gene_set : list\n",
    "        List of Entrez gene IDs to sample from\n",
    "    n_boot : int\n",
    "        Number of bootstrap replicates\n",
    "    n_genes : int, optional\n",
    "        Number of genes to sample per bootstrap. If None, uses length of gene_set\n",
    "    rng : np.random.Generator, optional\n",
    "        Numpy random generator for reproducibility\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    boot_gene_sets : list of lists\n",
    "        List of bootstrap gene sets, each containing sampled Entrez IDs\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "    \n",
    "    if n_genes is None:\n",
    "        n_genes = len(gene_set)\n",
    "    \n",
    "    gene_set_array = np.array(gene_set)\n",
    "    boot_gene_sets = []\n",
    "    \n",
    "    for i in range(1, n_boot + 1):\n",
    "        # Sample n_genes with replacement from gene_set\n",
    "        boot_genes = rng.choice(gene_set_array, size=n_genes, replace=True)\n",
    "        boot_gene_sets.append(boot_genes.tolist())\n",
    "    \n",
    "    return boot_gene_sets\n",
    "\n",
    "# Bootstrap genes from LOEUF25 gene set\n",
    "print(f\"Bootstrapping {len(LOEUF25_genes)} genes from LOEUF25 gene set...\")\n",
    "LOEUF25_boot_gene_sets = bootstrap_genes_from_set(LOEUF25_genes, n_boot=1000, rng=np.random.default_rng(42))\n",
    "print(f\"Created {len(LOEUF25_boot_gene_sets)} bootstrap replicates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process LOEUF25 bootstrap iterations in parallel\n",
    "def process_LOEUF25_bootstrap_iter(args):\n",
    "    \"\"\"Worker function to process a single LOEUF25 bootstrap iteration\"\"\"\n",
    "    i, boot_genes, save_dir, str_bias_mat = args\n",
    "    # Create gene weights dictionary (equal weights of 1)\n",
    "    boot_gw = {gene: 1.0 for gene in boot_genes}\n",
    "    boot_bias = MouseSTR_AvgZ_Weighted(str_bias_mat, boot_gw)\n",
    "    boot_bias.to_csv(os.path.join(save_dir, f\"LOEUF25.GeneWeight.boot{i}.csv\"))\n",
    "    return i, boot_bias\n",
    "\n",
    "save_dir_LOEUF25 = \"../results/Bootstrap_bias/LOEUF25/Weighted_Resampling\"\n",
    "os.makedirs(save_dir_LOEUF25, exist_ok=True)\n",
    "\n",
    "# Prepare arguments for parallel processing\n",
    "n_workers = mp.cpu_count()\n",
    "args_list_LOEUF25 = [(i, boot_genes, save_dir_LOEUF25, STR_BiasMat) \n",
    "                     for i, boot_genes in enumerate(LOEUF25_boot_gene_sets)]\n",
    "\n",
    "# Process in parallel\n",
    "print(f\"Using {n_workers} workers to compute LOEUF25 bootstrap bias...\")\n",
    "LOEUF25_boot_bias_list = [None] * len(LOEUF25_boot_gene_sets)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    future_to_idx = {executor.submit(process_LOEUF25_bootstrap_iter, args): args[0] \n",
    "                     for args in args_list_LOEUF25}\n",
    "    \n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_idx):\n",
    "        i, boot_bias = future.result()\n",
    "        LOEUF25_boot_bias_list[i] = boot_bias\n",
    "        completed += 1\n",
    "        if completed % 100 == 0:\n",
    "            print(f\"Completed {completed}/{len(LOEUF25_boot_gene_sets)} LOEUF25 bootstrap iterations\")\n",
    "\n",
    "print(f\"Completed all {len(LOEUF25_boot_bias_list)} LOEUF25 bootstrap iterations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residues for LOEUF25 bootstrap vs ASD bootstrap (parallelized)\n",
    "print(\"Computing residues for LOEUF25 bootstrap vs ASD observed...\")\n",
    "\n",
    "def compute_residuals_LOEUF25_vs_ASD(args):\n",
    "    \"\"\"Worker function to compute residuals for a single LOEUF25 bootstrap iteration vs ASD\"\"\"\n",
    "    i, loeuf25_boot, asd_bias = args\n",
    "    merged = merge_str_bias_datasets(asd_bias, loeuf25_boot, suffixes=('_ASD', '_LOEUF25'))\n",
    "    return i, merged['residual'].to_dict()\n",
    "\n",
    "# Prepare arguments for parallel processing\n",
    "args_list_residuals_LOEUF25 = [(i, loeuf25_boot, Spark_ASD_STR_Bias) \n",
    "                               for i, loeuf25_boot in enumerate(LOEUF25_boot_bias_list)]\n",
    "\n",
    "# Process in parallel\n",
    "print(f\"Using {n_workers} workers to compute LOEUF25 residuals...\")\n",
    "LOEUF25_residual_results = [None] * len(LOEUF25_boot_bias_list)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    future_to_idx = {executor.submit(compute_residuals_LOEUF25_vs_ASD, args): args[0] \n",
    "                     for args in args_list_residuals_LOEUF25}\n",
    "    \n",
    "    completed = 0\n",
    "    for future in as_completed(future_to_idx):\n",
    "        i, residuals_dict = future.result()\n",
    "        LOEUF25_residual_results[i] = residuals_dict\n",
    "        completed += 1\n",
    "        if completed % 100 == 0:\n",
    "            print(f\"Completed {completed}/{len(LOEUF25_boot_bias_list)} LOEUF25 residual computations\")\n",
    "\n",
    "# Collect all residues for each structure\n",
    "LOEUF25_structure_residuals = {}\n",
    "for residuals_dict in LOEUF25_residual_results:\n",
    "    for structure, residual in residuals_dict.items():\n",
    "        if structure not in LOEUF25_structure_residuals:\n",
    "            LOEUF25_structure_residuals[structure] = []\n",
    "        LOEUF25_structure_residuals[structure].append(residual)\n",
    "\n",
    "print(f\"Computed residues for {len(LOEUF25_structure_residuals)} structures across {len(LOEUF25_boot_bias_list)} bootstrap iterations\")\n",
    "\n",
    "# Calculate 95% CI for each structure\n",
    "LOEUF25_residual_ci = {}\n",
    "for structure, residuals in LOEUF25_structure_residuals.items():\n",
    "    residuals_array = np.array(residuals)\n",
    "    ci_lower = np.percentile(residuals_array, 2.5)\n",
    "    ci_upper = np.percentile(residuals_array, 97.5)\n",
    "    median_residual = np.median(residuals_array)\n",
    "    mean_residual = np.mean(residuals_array)\n",
    "    \n",
    "    LOEUF25_residual_ci[structure] = {\n",
    "        'mean': mean_residual,\n",
    "        'median': median_residual,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'std': np.std(residuals_array)\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "LOEUF25_residual_ci_df = pd.DataFrame(LOEUF25_residual_ci).T\n",
    "LOEUF25_residual_ci_df = LOEUF25_residual_ci_df.sort_values('median', ascending=False)\n",
    "LOEUF25_residual_ci_df.index.name = 'Structure'\n",
    "\n",
    "print(\"\\n95% CI of LOEUF25 bias residues computed for all structures\")\n",
    "print(f\"\\nTop 10 structures by median residual:\")\n",
    "print(LOEUF25_residual_ci_df.head(10))\n",
    "\n",
    "# Save results\n",
    "save_dir_LOEUF25_ci = \"../results/Bootstrap_bias/LOEUF25/Residual_CI\"\n",
    "os.makedirs(save_dir_LOEUF25_ci, exist_ok=True)\n",
    "LOEUF25_residual_ci_df.to_csv(os.path.join(save_dir_LOEUF25_ci, \"LOEUF25.Residual_CI_95.csv\"))\n",
    "print(f\"\\nResults saved to: {save_dir_LOEUF25_ci}/LOEUF25.Residual_CI_95.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed LOEUF25 bias for comparison\n",
    "# IMPORTANT: Match DDD.ipynb exactly - use all Entrez IDs from gnomad4_bottom25 (not just unique)\n",
    "# In DDD.ipynb: constraint_gw_top_LOEUF25 = dict(zip(gnomad4_bottom25[\"Entrez\"], [1]*len(gnomad4_bottom25)))\n",
    "# This ensures we get the exact same gene weights as DDD.ipynb\n",
    "LOEUF25_gw_observed = dict(zip(gnomad4_bottom25[\"Entrez\"], [1]*len(gnomad4_bottom25)))\n",
    "LOEUF25_STR_Bias_observed = MouseSTR_AvgZ_Weighted(STR_BiasMat, LOEUF25_gw_observed)\n",
    "LOEUF25_STR_Bias_observed[\"Region\"] = [Anno.get(ct_idx, \"Unknown\") for ct_idx in LOEUF25_STR_Bias_observed.index.values]\n",
    "\n",
    "# Merge observed data for LOEUF25 vs ASD\n",
    "merged_data_LOEUF25_observed = merge_str_bias_datasets(Spark_ASD_STR_Bias, LOEUF25_STR_Bias_observed, \n",
    "                                                       suffixes=('_ASD', '_LOEUF25'))\n",
    "\n",
    "# Filter to circuit structures if available\n",
    "if Circuit_STRs is not None:\n",
    "    merged_data_LOEUF25_eval = merged_data_LOEUF25_observed[merged_data_LOEUF25_observed.index.isin(Circuit_STRs)]\n",
    "else:\n",
    "    merged_data_LOEUF25_eval = merged_data_LOEUF25_observed.copy()\n",
    "\n",
    "# Create the plot with 95% CI\n",
    "top_diff_LOEUF25 = plot_top_residual_structures_with_CI(merged_data_LOEUF25_eval, LOEUF25_residual_ci_df, \n",
    "                                                        top_n=20, top_threshold=40,\n",
    "                                                        name1=\"ASD\", name2=\"LOEUF25\", \n",
    "                                                        figsize=(6, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gencic)",
   "language": "python",
   "name": "gencic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

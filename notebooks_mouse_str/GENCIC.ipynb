{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook demonstrating CENCIC algorithm for circuit search with using ASD mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement data file:\n",
    "1. ASD mutation bias file: \"Spark_Meta_EWS.Z2.bias.FDR.csv\"\n",
    "2. Information Score for connetome \"Spark_Meta_EWS.Z2.info.csv\"\n",
    "\n",
    "Scripts used in this notebook:\n",
    "1. script.Pareto.generate_bias_lim.py\n",
    "2. script.Pareto.generate_bias_lim.py\n",
    "3. script.Pareto.generate_bias_lim.py\n",
    "4. script.Pareto.generate_bias_lim.py\n",
    "5. script.Pareto.generate_bias_lim.py\n",
    "6. script.Pareto.generate_bias_lim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "RootDIR = \"/home/jw3514/Work/ASD_Circuits_CellType/\" # put this in the right place\n",
    "os.chdir(RootDIR + \"/notebooks_mouse_str\") # put this in the right place\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "sys.path.insert(1, RootDIR + 'src')\n",
    "# Need to add src directory to Python path first\n",
    "\n",
    "#sys.path.append(\"../src\")\n",
    "from ASD_Circuits import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Calculate mutation strcture biases with ASD mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark_ASD_STR_Bias = pd.read_csv(\"../dat/Unionize_bias/Spark_Meta_EWS.Z2.bias.FDR.csv\", index_col=0)\n",
    "Spark_ASD_STR_Bias.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate CCS scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Circuit Search with SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Calculate Biaslim for SA search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate bias limits for different circuit sizes. In our paper we use size 46 as main search size since it has highest CCS score.\n",
    "\n",
    "The bias step size is 0.005 when bias > 0.3, 0.01 when bias > 0.2 and 0.05 when bias <= 0.2, to decrease computation burden. (we care less about the bias limits for low bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bias limits for different circuit sizes\n",
    "OutDIR = \"../dat/CircuitSearch/Biaslims/\"\n",
    "BiasDF = Spark_ASD_STR_Bias\n",
    "\n",
    "sizes = np.arange(10, 100, 1)\n",
    "for i, t in enumerate(sizes):\n",
    "    fout = open(OutDIR + \"biaslim.size.{}.txt\".format(t), 'w')\n",
    "    writer = csv.writer(fout)\n",
    "    lims = BiasLim(BiasDF, t)\n",
    "    for size, bias in lims:\n",
    "        writer.writerow([size, bias])\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selected_BiasLim = pd.read_csv(OutDIR + \"biaslim.size.46.txt\", names=[\"size\", \"bias\"])\n",
    "Selected_BiasLim = Selected_BiasLim[Selected_BiasLim[\"bias\"] >= 0.3] # select bias >= 0.3 to reduce number of jobs\n",
    "Selected_BiasLim.reset_index(inplace=True, drop=True)\n",
    "Selected_BiasLim.to_csv(OutDIR + \"biaslim.size.46.top17.txt\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selected_BiasLim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 run bash script to search circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circuit Search Using Simulated Annealing\n",
    "Now we can run the bash script `scripts/submit_job_run_pareto.SI.sh` to search for circuits using the selected bias limits.\n",
    "\n",
    "This is a computationally intensive process that may take 1-2 days to complete, depending on:\n",
    "- Number of parallel threads used\n",
    "- Size of the search space\n",
    "- Number of bias limits being explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of files/variables used in the bash script:\n",
    "- `BiasDF=../dat/Unionize_bias/Spark_Meta_EWS.Z2.bias.FDR.csv`: ASD mutation bias file\n",
    "- `AdjMat=../dat/allen-mouse-conn/ScoreingMat_jw_v3/WeightMat.Ipsi.csv`: Connection weights for connetome\n",
    "- `InfoMat=../dat/allen-mouse-conn/ScoreingMat_jw_v3/InfoMat.Ipsi.csv`: Information Score for connetome\n",
    "- `BiasLim=../dat/CircuitSearch/Biaslims/biaslim.size.46.txt`: Bias limits for different circuit sizes\n",
    "- `DIR=../dat/CircuitSearch/results/ASD_Pareto_46`:  Output directory\n",
    "- `NJob`: Number of total searches to complete the Pareto front (number of bias limits), calculated using `wc -l $BiasLim | cut -f 1 -d ' '`\n",
    "- `Nparallel=20`: Number of parallel threads used\n",
    "\n",
    "Fill the variables and run the bash script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Collect results and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normtoUnit(x, xmin, xmax):\n",
    "    return (x-xmin)/(xmax-xmin)\n",
    "\n",
    "def searchFil(text, DIR):\n",
    "    #print(text)\n",
    "    RES = []\n",
    "    for file in os.listdir(DIR):\n",
    "        if text in file:\n",
    "            RES.append(file)\n",
    "    return RES\n",
    "\n",
    "def LoadSA3(fname, DIR, InfoMat, minbias, topL=100):\n",
    "    fin = open(DIR+fname, 'rt')\n",
    "    max_score, max_bias, max_STRs = 0, 0, []\n",
    "    for i, l in enumerate(fin):\n",
    "        if i > topL:\n",
    "            break\n",
    "        l = l.strip().split()\n",
    "        bias = float(l[1])\n",
    "        if bias < minbias:\n",
    "            continue\n",
    "        STRs = l[2].split(\",\")\n",
    "        score = ScoreCircuit_SI_Joint(STRs, InfoMat)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            max_bias = bias\n",
    "            max_STRs = STRs\n",
    "    return max_score, max_bias, max_STRs\n",
    "\n",
    "def GetData2(params, size, DIR, adj_mat, InfoMat):\n",
    "    SCORES, CutBias, RealBias, STRS = [],[],[],[]\n",
    "    for i, row in params.iterrows():\n",
    "        fil = searchFil(\"keepN_{}-minbias_{}.txt\".format(size, row[\"bias\"]), DIR)[0]\n",
    "        score, real_minbias, STRs = LoadSA3(fil, DIR, InfoMat, row[\"bias\"])\n",
    "        score = ScoreCircuit_SI_Joint(STRs, InfoMat)\n",
    "        if score == 0:\n",
    "            continue\n",
    "        SCORES.append(score)\n",
    "        CutBias.append(row[\"bias\"])\n",
    "        RealBias.append(real_minbias)\n",
    "        STRS.append(STRs)\n",
    "    return SCORES, CutBias, RealBias, STRS\n",
    "\n",
    "def XXXX_cont(BiasDF, BiasDF2, biaslim_df, size, DIR, adj_mat, InfoMat):\n",
    "    #fil = searchFil(\"keepN_{}-minbias_{}.txt\".format(size, bias), DIR)[0]\n",
    "    SCORES, CutBias, RealBias, STRS = GetData2(biaslim_df, size, DIR, adj_mat, InfoMat)\n",
    "    New_RealBias = []\n",
    "    for STRSET in STRS:\n",
    "        xx = BiasDF.loc[STRSET, \"EFFECT\"].mean()\n",
    "        New_RealBias.append(xx)\n",
    "    # Add top size STRs\n",
    "    topNSTRs = BiasDF.index.values[:size]\n",
    "    bias = BiasDF.head(size)[\"EFFECT\"].mean()\n",
    "    score = ScoreCircuit_SI_Joint(topNSTRs, InfoMat)\n",
    "    SCORES.append(score)\n",
    "    CutBias.append(bias)\n",
    "    New_RealBias.append(bias)\n",
    "    STRS.append(topNSTRs)    \n",
    "    return SCORES, CutBias, New_RealBias, STRS\n",
    "\n",
    "def search_target_swap(size, BiasDF, NSwap, biaslim_df, adj_mat, \n",
    "                       ProbMat1, ProbMat1_short, ProbMat1_long, \n",
    "                       ProbMat2, ProbMat2_short, ProbMat2_long, DIR):\n",
    "    # TopN targets \n",
    "    topNSTRs = BiasDF.index.values[:size]\n",
    "    bias = BiasDF.head(size)[\"EFFECT\"].mean()\n",
    "    score = ScoreCircuit_v7(topNSTRs, adj_mat, ProbMat1, ProbMat2)\n",
    "    # search along the profile\n",
    "    for i, row in biaslim_df.iterrows():\n",
    "        fil = searchFil(\"keepN_{}-minbias_{}.txt\".format(size, row[\"bias\"]), DIR)[0]\n",
    "        cohe, real_minbias, STRs = LoadSA3(fil, DIR, adj_mat, ProbMat1, ProbMat2)\n",
    "        score = ScoreCircuit_v7(STRs, adj_mat, ProbMat1, ProbMat2)\n",
    "\n",
    "        bias = BiasDF.loc[STRs, \"EFFECT\"].mean()\n",
    "        NDiff = len(set(STRs).difference(topNSTRs))\n",
    "        if abs(NDiff-NSwap) < 2:\n",
    "\n",
    "            score1 = ScoreCircuit_v7(STRs, adj_mat, ProbMat1_short, ProbMat2_short)\n",
    "            score2 = ScoreCircuit_v7(STRs, adj_mat, ProbMat1_long, ProbMat2_long)\n",
    "            if score > 0.714:\n",
    "                #print(RegionDistributionsList(STRs))\n",
    "                print(score, score1, score2)\n",
    "            return bias, score, score1, score2\n",
    "    return None, None, None, None\n",
    "\n",
    "def search_target_swap2(size, BiasDF, biaslim, biaslim_df, adj_mat, \n",
    "                       ProbMat1, ProbMat1_short, ProbMat1_long, \n",
    "                       ProbMat2, ProbMat2_short, ProbMat2_long, DIR):\n",
    "    # TopN targets \n",
    "    topNSTRs = BiasDF.index.values[:size]\n",
    "    bias = BiasDF.head(size)[\"EFFECT\"].mean()\n",
    "    score = ScoreCircuit_v7(topNSTRs, adj_mat, ProbMat1, ProbMat2)\n",
    "    # search along the profile\n",
    "    for i, row in biaslim_df.iterrows():\n",
    "        fil = searchFil(\"keepN_{}-minbias_{}.txt\".format(size, row[\"bias\"]), DIR)[0]\n",
    "        cohe, real_minbias, STRs = LoadSA3(fil, DIR, adj_mat, ProbMat1, ProbMat2)\n",
    "        score = ScoreCircuit_v7(STRs, adj_mat, ProbMat1, ProbMat2)\n",
    "\n",
    "        bias = BiasDF.loc[STRs, \"EFFECT\"].mean()\n",
    "        #print(round(real_minbias,3), biaslim)\n",
    "        if round(real_minbias,3) == biaslim:\n",
    "            score1 = ScoreCircuit_v7(STRs, adj_mat, ProbMat1_short, ProbMat2_short)\n",
    "            score2 = ScoreCircuit_v7(STRs, adj_mat, ProbMat1_long, ProbMat2_long)\n",
    "            if score2 > 0.673:\n",
    "                #print()\n",
    "                print(RegionDistributionsList(STRs))\n",
    "            return bias, score, score1, score2\n",
    "    return None, None, None, None\n",
    "\n",
    "def LoadProfiles(BiasDF, BiasDF2, biaslim_df, size, DIR, adj_mat, InfoMat):\n",
    "    Scores, CutBias, RealBias, STRS = GetData2(biaslim_df, size, DIR, adj_mat, InfoMat)\n",
    "    # Add top size STRs\n",
    "    topNSTRs = BiasDF.index.values[:size]\n",
    "    bias = BiasDF2.head(size)[\"EFFECT\"].mean()\n",
    "    score = ScoreCircuit_SI_Joint(topNSTRs, InfoMat)\n",
    "    Scores.append(score)\n",
    "    CutBias.append(bias)\n",
    "    RealBias.append(bias)\n",
    "    STRS.append(topNSTRs)    \n",
    "    return Scores, CutBias, RealBias, STRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read connectome files\n",
    "InfoMat = pd.read_csv(\"../dat/allen-mouse-conn/ConnectomeScoringMat/InfoMat.Ipsi.csv\", index_col=0)\n",
    "adj_mat = pd.read_csv(\"../dat/allen-mouse-conn/ConnectomeScoringMat/WeightMat.Ipsi.csv\", index_col=0)\n",
    "InfoMat_short = pd.read_csv(\"../dat/allen-mouse-conn/ConnectomeScoringMat/InfoMat.Ipsi.short.csv\", index_col=0)\n",
    "InfoMat_long = pd.read_csv(\"../dat/allen-mouse-conn/ConnectomeScoringMat/InfoMat.Ipsi.long.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 46\n",
    "#biaslim_df = pd.read_csv(biaslim_dir + \"biaslim.size.{}.txt\".format(size), names=[\"size\", \"bias\"])\n",
    "ASD_DIR = \"../dat/CircuitSearch/SA/ASD_Pareto_SI_Size46/\"\n",
    "ASD_BiasDF = Spark_ASD_STR_Bias\n",
    "biaslim_df = pd.read_csv(OutDIR + \"biaslim.size.46.top17.txt\")\n",
    "COHESPeak, CutBiasPeak, RealBiasPeak, STRSPeak = LoadProfiles(ASD_BiasDF, ASD_BiasDF, biaslim_df, size, \n",
    "                                              ASD_DIR, adj_mat, InfoMat)\n",
    "ASD_DFPeak = pd.DataFrame(data={\"Cohe\":COHESPeak, \"minBias\":CutBiasPeak, \"Bias\":RealBiasPeak})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=120, figsize=(5,5))\n",
    "plt.plot(ASD_DFPeak[\"Cohe\"].values, ASD_DFPeak[\"Bias\"].values, marker=\".\", color=\"#542788\",  lw=2, markersize=8,\n",
    "             ls = \"-\", label=\"ASD\")\n",
    "plt.scatter(ASD_DFPeak[\"Cohe\"].values[-3], ASD_DFPeak[\"Bias\"].values[-3], marker=\"x\", s=50, color=\"red\",\n",
    "           zorder=100, label=\"Selected Circuits\")\n",
    "\n",
    "plt.xlabel(\"Circuit Score\")\n",
    "plt.ylabel(\"Mean Structure bias\")\n",
    "plt.grid()\n",
    "plt.ylim((0.05, 0.4))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the selected circuits\n",
    "print(RegionDistributionsList(STRSPeak[-3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting ASD and Sibling Circuit Data\n",
    "\n",
    "The following analysis compares ASD circuits with sibling control data.\n",
    "\n",
    "**Note:** The sibling data shown here is for visualization purposes only. The full analysis used data generated by running simulated annealing (SA) search with the same procedure on 10,000 subsampled sibling sets, which is too large to include here.\n",
    "\n",
    "To generate your own sibling data:\n",
    "1. Use the bash script `scripts/submit_job_run_pareto.SI.sh` \n",
    "2. Run it with different sibling sets as input\n",
    "\n",
    "For details on the original procedure used to generate these profiles, see `Optimized_Circuits_Information_Score.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables from numpy file\n",
    "sibling_data = np.load('../dat/CircuitSearch/SA/ASD_Pareto_SI_Size46/circuit_analysis_data.sibling.SA.npz')\n",
    "meanbias = sibling_data['meanbias']\n",
    "meanSI = sibling_data['meanSI']\n",
    "topbias_sub = sibling_data['topbias_sub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=480, figsize=(4.2,4))\n",
    "\n",
    "ax.plot(ASD_DFPeak[\"Cohe\"].values, ASD_DFPeak[\"Bias\"].values, marker=\".\", color=\"#542788\",  lw=2, markersize=8,\n",
    "             ls = \"-\", label=\"ASD\")\n",
    "ax.scatter(ASD_DFPeak[\"Cohe\"].values[-4], ASD_DFPeak[\"Bias\"].values[-4], marker=\"x\", s=70, color=\"red\", lw=2,\n",
    "           zorder=100)\n",
    "ax.text(ASD_DFPeak[\"Cohe\"].values[-4], 0.01 + ASD_DFPeak[\"Bias\"].values[-4], s=\"Selected\\n Circuit\")\n",
    "\n",
    "ax.plot(topbias_sub[:,0,:].T, topbias_sub[:,1,:].T, color=\"grey\", markersize=1, lw=0.5,\n",
    "             ls = \"-\", alpha=0.05)\n",
    "#ax.plot(topbias_sub[0,0,:].T, topbias_sub[0,1,:].T, color=\"grey\", markersize=1, lw=1,\n",
    "#             ls = \"-\", alpha=1, label=\"Sibling Circuit\")\n",
    "\n",
    "ax.plot(meanSI, meanbias, marker=\".\", color=\"Orange\", lw=2, markersize=8,\n",
    "             ls = \"-\", alpha=1, label=\"Average Sibling Circuit\")\n",
    "ax.plot(meanSI, meanbias, color=\"grey\", lw=2, markersize=8,\n",
    "             ls = \"-\", alpha=1, label=\"Sibling Circuit\", zorder=0)\n",
    "\n",
    "#box = ax.get_position()\n",
    "#ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "#ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.9))\n",
    "ax.legend(loc=\"lower left\", frameon=False)\n",
    "\n",
    "plt.xlabel(\"Circuit Connectivity Score\", fontsize=14)\n",
    "plt.ylabel(\"Average Mutation Bias\", fontsize=14)\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.ylim(0.05, 0.42)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Annoate resulting circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

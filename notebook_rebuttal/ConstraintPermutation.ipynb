{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Permutation Test: ASD vs Constrained Gene Pool\n",
    "\n",
    "Tests whether ASD structure bias is significantly different from what would\n",
    "be expected by randomly sampling genes from the constrained gene pool\n",
    "(LOEUF top 10%).\n",
    "\n",
    "**Analyses:**\n",
    "1. Batch permutation (10K) with caching\n",
    "2. Per-structure null tests (e.g., Nucleus accumbens, Caudoputamen)\n",
    "3. Correlation null distribution (ASD and DDD vs random constrained subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "ProjDIR = \"/home/jw3514/Work/ASD_Circuits_CellType/\"\n",
    "sys.path.insert(1, f'{ProjDIR}/src/')\n",
    "from ASD_Circuits import *\n",
    "from plot import *\n",
    "\n",
    "try:\n",
    "    os.chdir(f\"{ProjDIR}/notebook_rebuttal/\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "HGNC, ENSID2Entrez, GeneSymbol2Entrez, Entrez2Symbol = LoadGeneINFO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config and expression matrices\n",
    "with open(\"../config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "STR_BiasMat = pd.read_parquet(f\"../{config['analysis_types']['STR_ISH']['expr_matrix']}\")\n",
    "STR_Anno = STR2Region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ASD bias and gene weights\n",
    "Spark_ASD_STR_Bias = pd.read_csv(\"../dat/Unionize_bias/Spark_Meta_EWS.Z2.bias.FDR.csv\", index_col=0)\n",
    "Spark_ASD_STR_Bias[\"Region\"] = Spark_ASD_STR_Bias[\"REGION\"]\n",
    "ASD_GW = Fil2Dict(ProjDIR + \"dat/Genetics/GeneWeights_DN/Spark_Meta_EWS.GeneWeight.DN.gw\")\n",
    "ASD_GENES = list(ASD_GW.keys())\n",
    "\n",
    "# Load DDD bias (exclude ASD genes)\n",
    "DDD_GW = Fil2Dict(config[\"gene_sets\"][\"DDD_293\"][\"geneweights\"])\n",
    "DDD_GW_filt_ASD = {k: v for k, v in DDD_GW.items() if k not in ASD_GENES}\n",
    "DDD_rmASD_STR_Bias = MouseSTR_AvgZ_Weighted(STR_BiasMat, DDD_GW_filt_ASD)\n",
    "DDD_rmASD_STR_Bias[\"Region\"] = [STR_Anno.get(s, \"Unknown\") for s in DDD_rmASD_STR_Bias.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gnomAD v4 constraint data\n",
    "gnomad4 = pd.read_csv(\"/home/jw3514/Work/data/gnomad/gnomad.v4.0.constraint_metrics.tsv\", sep=\"\\t\")\n",
    "gnomad4 = gnomad4[(gnomad4[\"transcript\"].str.contains('ENST'))]\n",
    "gnomad4 = gnomad4[gnomad4[\"mane_select\"] == True]\n",
    "for i, row in gnomad4.iterrows():\n",
    "    gnomad4.loc[i, \"Entrez\"] = int(GeneSymbol2Entrez.get(row[\"gene\"], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1. Batch permutation (with caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOEUF top 10% gene pool for permutation\n",
    "bottom_10_percent_threshold = gnomad4[\"lof.oe_ci.upper\"].quantile(0.1)\n",
    "gnomad4_bottom10 = gnomad4[gnomad4[\"lof.oe_ci.upper\"] <= bottom_10_percent_threshold]\n",
    "gnomad4_bottom10 = gnomad4_bottom10[[\"Entrez\", \"gene\", \"lof.pLI\", \"lof.z_score\", \"lof.oe_ci.upper\"]].copy()\n",
    "gnomad4_bottom10[\"Entrez\"] = gnomad4_bottom10[\"Entrez\"].astype(int)\n",
    "gnomad4_bottom10 = gnomad4_bottom10[gnomad4_bottom10[\"Entrez\"] != 0]\n",
    "gnomad4_bottom10 = gnomad4_bottom10.sort_values(by=\"lof.oe_ci.upper\", ascending=True)\n",
    "print(f\"LOEUF top 10% genes: {gnomad4_bottom10.shape[0]}\")\n",
    "\n",
    "constraint_gw = dict(zip(gnomad4_bottom10[\"Entrez\"], 1 / gnomad4_bottom10[\"lof.oe_ci.upper\"]))\n",
    "Geneset = list(constraint_gw.keys())\n",
    "Weights = list(ASD_GW.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"../results/cache/DDD_constraint_permutation_10K.pkl\"\n",
    "os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    print(\"Loading cached permutation results...\")\n",
    "    with open(cache_path, \"rb\") as f:\n",
    "        tmp_bias_dfs = pickle.load(f)\n",
    "    print(f\"Loaded {len(tmp_bias_dfs)} permutations from cache\")\n",
    "else:\n",
    "    print(\"Running 10K permutations (batch mode)...\")\n",
    "    tmp_bias_dfs = batch_permutation_bias(STR_BiasMat, Geneset, Weights, n_perm=10000, seed=42)\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(tmp_bias_dfs, f)\n",
    "    print(f\"Saved {len(tmp_bias_dfs)} permutations to cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2. Per-structure null tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value, observed_effect, null_effects = plot_null_distribution_analysis(\"Nucleus_accumbens\", tmp_bias_dfs, Spark_ASD_STR_Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value, observed_effect, null_effects = plot_null_distribution_analysis(\"Caudoputamen\", tmp_bias_dfs, Spark_ASD_STR_Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for all structures\n",
    "P_constraint = {}\n",
    "for structure in Spark_ASD_STR_Bias.index:\n",
    "    p_value, observed_effect, null_effects = plot_null_distribution_analysis(\n",
    "        structure, tmp_bias_dfs, Spark_ASD_STR_Bias, title_prefix=\"\", plot=False)\n",
    "    P_constraint[structure] = p_value\n",
    "\n",
    "Spark_ASD_STR_Bias_with_p = Spark_ASD_STR_Bias.copy()\n",
    "Spark_ASD_STR_Bias_with_p['P_constraint'] = Spark_ASD_STR_Bias_with_p.index.map(P_constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark_ASD_STR_Bias_with_p[Spark_ASD_STR_Bias_with_p[\"P_constraint\"] < 0.05].sort_values(by=\"P_constraint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark_ASD_STR_Bias_with_p[Spark_ASD_STR_Bias_with_p[\"P_constraint\"] > 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value, observed_effect, null_effects = plot_null_distribution_analysis(\"Facial_motor_nucleus\", tmp_bias_dfs, Spark_ASD_STR_Bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 3. Correlation null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-50 average EFFECT null\n",
    "records = [tmp_bias_dfs[i].head(50)[\"EFFECT\"].mean() for i in range(len(tmp_bias_dfs))]\n",
    "null_effects = np.array(records)\n",
    "observed_effect = Spark_ASD_STR_Bias.head(50)[\"EFFECT\"].mean()\n",
    "p_value = (np.sum(null_effects >= observed_effect) + 1) / (len(null_effects) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(null_effects, bins=50, alpha=0.7, color='lightblue', edgecolor='black', label='Null distribution (Constrained Genes)')\n",
    "plt.axvline(observed_effect, color='red', linestyle='--', linewidth=2, label=f'Observed (Spark ASD): {observed_effect:.4f}')\n",
    "plt.xlabel('EFFECT')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.text(0.05, 0.95, f'P-value: {p_value:.4f}', transform=plt.gca().transAxes,\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "plt.show()\n",
    "print(f\"Observed Spark ASD effect: {observed_effect:.4f}\")\n",
    "print(f\"Null mean: {np.mean(null_effects):.4f}, Null std: {np.std(null_effects):.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation null: ASD and DDD vs random constrained subsets\n",
    "Corrs_ASD_Constraint = []\n",
    "Corrs_DDD_Constraint = []\n",
    "for i in range(len(tmp_bias_dfs)):\n",
    "    top_avg_bias = tmp_bias_dfs[i]\n",
    "\n",
    "    tmp_merged = merge_bias_datasets(Spark_ASD_STR_Bias, top_avg_bias, suffixes=('_ASD', '_Constrained'))\n",
    "    Corrs_ASD_Constraint.append(tmp_merged[\"EFFECT_ASD\"].corr(tmp_merged[\"EFFECT_Constrained\"]))\n",
    "\n",
    "    tmp_merged = merge_bias_datasets(DDD_rmASD_STR_Bias, top_avg_bias, suffixes=('_DD', '_Constrained'))\n",
    "    Corrs_DDD_Constraint.append(tmp_merged[\"EFFECT_DD\"].corr(tmp_merged[\"EFFECT_Constrained\"]))\n",
    "\n",
    "Corrs_ASD_Constraint = np.array(Corrs_ASD_Constraint)\n",
    "Corrs_DDD_Constraint = np.array(Corrs_DDD_Constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute observed correlations from data (not hard-coded)\n",
    "constraint_STR_Bias = MouseSTR_AvgZ_Weighted(STR_BiasMat, constraint_gw)\n",
    "constraint_STR_Bias[\"Region\"] = [STR_Anno.get(s, \"Unknown\") for s in constraint_STR_Bias.index]\n",
    "\n",
    "merged_obs_asd = merge_bias_datasets(Spark_ASD_STR_Bias, constraint_STR_Bias, suffixes=('_ASD', '_Constrained'))\n",
    "observed_effect_asd = pearsonr(merged_obs_asd[\"EFFECT_ASD\"], merged_obs_asd[\"EFFECT_Constrained\"])[0]\n",
    "\n",
    "merged_obs_ddd = merge_bias_datasets(DDD_rmASD_STR_Bias, constraint_STR_Bias, suffixes=('_DD', '_Constrained'))\n",
    "observed_effect_ddd = pearsonr(merged_obs_ddd[\"EFFECT_DD\"], merged_obs_ddd[\"EFFECT_Constrained\"])[0]\n",
    "\n",
    "print(f\"Observed ASD vs Constraint correlation: {observed_effect_asd:.4f}\")\n",
    "print(f\"Observed DDD vs Constraint correlation: {observed_effect_ddd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot null distribution vs observed for both ASD and DDD\n",
    "null_effects_asd = Corrs_ASD_Constraint\n",
    "p_value_asd = (np.sum(null_effects_asd >= observed_effect_asd) + 1) / (len(null_effects_asd) + 1)\n",
    "\n",
    "null_effects_ddd = Corrs_DDD_Constraint\n",
    "p_value_ddd = (np.sum(null_effects_ddd >= observed_effect_ddd) + 1) / (len(null_effects_ddd) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(null_effects_asd, bins=50, alpha=0.7, color='lightblue', edgecolor='black', label='Null distribution (Constrained Genes)')\n",
    "ax.axvline(observed_effect_asd, color='red', linestyle='--', linewidth=2, label=f'Observed (Spark ASD): {observed_effect_asd:.4f}')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(0.05, 0.95, f'P-value: {p_value_asd:.4f}', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), va='top')\n",
    "ax.set_title('ASD (Spark): Correlation Null Distribution vs Observed')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.hist(null_effects_ddd, bins=50, alpha=0.7, color='lightgreen', edgecolor='black', label='Null distribution (Constrained Genes)')\n",
    "ax.axvline(observed_effect_ddd, color='red', linestyle='--', linewidth=2, label=f'Observed (DDD): {observed_effect_ddd:.4f}')\n",
    "ax.set_xlabel('EFFECT')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(0.05, 0.95, f'P-value: {p_value_ddd:.4f}', transform=ax.transAxes,\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), va='top')\n",
    "ax.set_title('DDD: Correlation Null Distribution vs Observed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ASD: observed={observed_effect_asd:.4f}, null mean={np.mean(null_effects_asd):.4f}, std={np.std(null_effects_asd):.4f}, P={p_value_asd:.4f}\")\n",
    "print(f\"DDD: observed={observed_effect_ddd:.4f}, null mean={np.mean(null_effects_ddd):.4f}, std={np.std(null_effects_ddd):.4f}, P={p_value_ddd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "gencic",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
